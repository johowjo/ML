{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 8: Model Editing\n",
        "This is the code for the homework 8. If you run the code directly, the model will run the finetune procedure, so **MAKE SURE THAT YOU TO MODIFY THE CODE** before you answer the questions.  \n",
        "This codebook is modified from the repo: **https://github.com/kmeng01/memit**.\n"
      ],
      "metadata": {
        "id": "cLFY5Umdmn5W"
      },
      "id": "cLFY5Umdmn5W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference:\n",
        "* https://github.com/kmeng01/rome\n",
        "* https://github.com/kmeng01/memit\n",
        "* https://arxiv.org/pdf/2202.05262\n",
        "* https://arxiv.org/pdf/2110.11309\n",
        "* https://arxiv.org/pdf/2210.07229"
      ],
      "metadata": {
        "id": "GYoVjgEn5sE7"
      },
      "id": "GYoVjgEn5sE7"
    },
    {
      "cell_type": "markdown",
      "id": "SqBhhuHN0Y_7",
      "metadata": {
        "id": "SqBhhuHN0Y_7"
      },
      "source": [
        "# Environment Setup\n",
        "Here we'll download & import the package and the MEMIT repository for their utility function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "MrMgGV4tPUr4",
      "metadata": {
        "id": "MrMgGV4tPUr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "960cdca8-a617-4d82-feb8-8bd1010ab82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'memit'...\n",
            "remote: Enumerating objects: 196, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 196 (delta 34), reused 29 (delta 29), pack-reused 124 (from 1)\u001b[K\n",
            "Receiving objects: 100% (196/196), 135.34 KiB | 2.42 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ]
        }
      ],
      "source": [
        "# Download MEMIT repository\n",
        "!cd /content\n",
        "!rm -rf /content/memit\n",
        "!git clone https://github.com/kmeng01/memit memit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "LtfrB_QQSFpA",
      "metadata": {
        "id": "LtfrB_QQSFpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d7ab49-356a-482b-f608-cdce61f0bc02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Collecting torch==2.5.1\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torch-2.5.1%2Bcu124-cp311-cp311-linux_x86_64.whl (908.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.20.1\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.20.1%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.5.1\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.5.1%2Bcu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch==2.5.1)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
            "Installing collected packages: triton, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.5.1+cu124 torchaudio-2.5.1+cu124 torchvision-0.20.1+cu124 triton-3.1.0\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n",
            "Requirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.11/dist-packages (0.31.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.13.2)\n",
            "Collecting hf-xet<2.0.0,>=1.1.1 (from huggingface_hub[hf_xet])\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2025.4.26)\n",
            "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf-xet\n",
            "Successfully installed hf-xet-1.1.2\n",
            "Collecting hydra-core\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting higher\n",
            "  Downloading higher-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.11/dist-packages (from hydra-core) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hydra-core) (24.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from higher) (2.5.1+cu124)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->higher) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->higher) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->higher) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->higher) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->higher) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->higher) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading higher-0.2.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: hydra-core, higher\n",
            "Successfully installed higher-0.2.1 hydra-core-1.3.2\n"
          ]
        }
      ],
      "source": [
        "# Important package download. This block will takes about 3 minutes\n",
        "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install datasets python-dotenv\n",
        "!pip install huggingface_hub[hf_xet]\n",
        "!pip install hydra-core higher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5odlxu6CScwj",
      "metadata": {
        "id": "5odlxu6CScwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14cf3656-ec9d-4c14-9ae8-6a5daed24fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/memit\n"
          ]
        }
      ],
      "source": [
        "%cd /content/memit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "zJxDsnH-YroW",
      "metadata": {
        "id": "zJxDsnH-YroW"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = False\n",
        "ALL_DEPS = False\n",
        "try:\n",
        "    import google.colab, torch, os\n",
        "\n",
        "    IS_COLAB = True\n",
        "except ModuleNotFoundError as _:\n",
        "    pass\n",
        "os.chdir(\"/content/memit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aec81909",
      "metadata": {
        "id": "aec81909",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Package import. Feel free to use\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import unicodedata\n",
        "from typing import Dict, List, Optional, Tuple, Union, Any\n",
        "from dataclasses import dataclass\n",
        "from copy import deepcopy\n",
        "import datasets\n",
        "import numpy as np\n",
        "\n",
        "from rome import repr_tools\n",
        "from util import nethook\n",
        "from util.globals import *\n",
        "from util.hparams import HyperParams\n",
        "from util.generate import generate_fast\n",
        "from rome.layer_stats import layer_stats\n",
        "import memit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset from drive\n",
        "!gdown 1UpOc2Yh_YdRhWW_cvEtawKlMIwEuCVvc -O /content/HW8_data.json"
      ],
      "metadata": {
        "id": "l_ZH8VNhfXdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87c3765-651c-4e87-e2c1-8e9e05108c06"
      },
      "id": "l_ZH8VNhfXdA",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UpOc2Yh_YdRhWW_cvEtawKlMIwEuCVvc\n",
            "To: /content/HW8_data.json\n",
            "\r  0% 0.00/75.4k [00:00<?, ?B/s]\r100% 75.4k/75.4k [00:00<00:00, 74.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V8aSKjbcXvkB",
      "metadata": {
        "id": "V8aSKjbcXvkB"
      },
      "source": [
        "# Predefined Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ib2lHuL7cmZR",
      "metadata": {
        "id": "ib2lHuL7cmZR"
      },
      "source": [
        "### Util Function\n",
        "Generation, basic model processing, printing and scoring. No need to be modified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "ExZLHP-1Xz52",
      "metadata": {
        "id": "ExZLHP-1Xz52"
      },
      "outputs": [],
      "source": [
        "def get_parameter(model, name):\n",
        "    \"\"\"\n",
        "    Finds the named parameter within the given model.\n",
        "    \"\"\"\n",
        "    for n, p in model.named_parameters():\n",
        "        if n == name:\n",
        "            return p\n",
        "    raise LookupError(name)\n",
        "\n",
        "def set_requires_grad(requires_grad, *models):\n",
        "    \"\"\"\n",
        "    Sets requires_grad true or false for all parameters within the\n",
        "    models passed.\n",
        "    \"\"\"\n",
        "    for model in models:\n",
        "        if isinstance(model, torch.nn.Module):\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = requires_grad\n",
        "        elif isinstance(model, (torch.nn.Parameter, torch.Tensor)):\n",
        "            model.requires_grad = requires_grad\n",
        "        else:\n",
        "            assert False, \"unknown type %r\" % type(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "2S_VfXPTZQ2J",
      "metadata": {
        "id": "2S_VfXPTZQ2J"
      },
      "outputs": [],
      "source": [
        "def generate(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    prompts: List[str],\n",
        "    n_gen_per_prompt: int = 1,\n",
        "    top_k: int = 5,\n",
        "    max_out_len: int = 200,\n",
        "    max_batch: int = 10,\n",
        "    first_do_sample: bool = True\n",
        "):\n",
        "    txts = []\n",
        "    for i in range((len(prompts)-1)//max_batch+1):\n",
        "        \"\"\"\n",
        "        The generated function with top K sampling. Feel free to adapt the code for top P and beam search!\n",
        "        \"\"\"\n",
        "        first_do_sample_inLoop = 10 if first_do_sample else 0\n",
        "        inp = [prompt for prompt in prompts[10*i:min(10*(i+1), len(prompts))] for _ in range(n_gen_per_prompt)]\n",
        "        inp_tok = tok(inp, padding=True, return_tensors=\"pt\").to(\n",
        "            next(model.parameters()).device\n",
        "        )\n",
        "        input_ids, attention_mask = inp_tok[\"input_ids\"], inp_tok[\"attention_mask\"]\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        past_key_values, cur_context = None, slice(0, attention_mask.sum(1).min().item())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            while input_ids.size(1) < max_out_len:  # while not exceeding max output length\n",
        "                model_out = model(\n",
        "                    input_ids=input_ids[:, cur_context],\n",
        "                    attention_mask=attention_mask[:, cur_context],\n",
        "                    past_key_values=past_key_values,\n",
        "                    use_cache=True,\n",
        "                )\n",
        "                logits, past_key_values = model_out.logits, model_out.past_key_values\n",
        "                softmax_out = torch.nn.functional.softmax(logits[:, -1, :], dim=1)\n",
        "\n",
        "                if first_do_sample_inLoop < 10:\n",
        "                    new_toks = torch.argmax(softmax_out, dim=1)\n",
        "                    first_do_sample_inLoop += 1\n",
        "                else:\n",
        "                    tk = torch.topk(softmax_out, top_k, dim=1).indices\n",
        "                    softmax_out_top_k = torch.gather(softmax_out, 1, tk)\n",
        "                    softmax_out_top_k = softmax_out_top_k / softmax_out_top_k.sum(1)[:, None]\n",
        "                    new_tok_indices = torch.multinomial(softmax_out_top_k, 1)\n",
        "                    new_toks = torch.gather(tk, 1, new_tok_indices)\n",
        "\n",
        "                if cur_context.stop == input_ids.size(1):\n",
        "                    attention_mask = torch.cat(\n",
        "                        [attention_mask, attention_mask.new_zeros(batch_size, 1)], dim=1\n",
        "                    )\n",
        "                    input_ids = torch.cat(\n",
        "                        [\n",
        "                            input_ids,\n",
        "                            input_ids.new_ones(batch_size, 1) * tok.pad_token_id,\n",
        "                        ],\n",
        "                        dim=1,\n",
        "                    )\n",
        "\n",
        "                last_non_masked = attention_mask.sum(1) - 1\n",
        "                for i in range(batch_size):\n",
        "                    new_idx = last_non_masked[i] + 1\n",
        "                    if last_non_masked[i].item() + 1 != cur_context.stop:\n",
        "                        continue\n",
        "\n",
        "                    # Stop generating if we've already maxed out for this prompt\n",
        "                    if new_idx < max_out_len:\n",
        "                        input_ids[i][new_idx] = new_toks[i]\n",
        "                        attention_mask[i][new_idx] = 1\n",
        "\n",
        "                cur_context = slice(cur_context.stop, cur_context.stop + 1)\n",
        "\n",
        "        txt = [tok.decode(x) for x in input_ids.detach().cpu().numpy().tolist()]\n",
        "        txt = [\n",
        "            unicodedata.normalize(\"NFKD\", x)\n",
        "            .replace(\"\\n\\n\", \" \")\n",
        "            .replace(\"<|endoftext|>\", \"\")\n",
        "            for x in txt\n",
        "        ]\n",
        "        txts += txt\n",
        "\n",
        "    return txts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "h7dDt_gwHWEl",
      "metadata": {
        "id": "h7dDt_gwHWEl"
      },
      "outputs": [],
      "source": [
        "def print_loud(x, pad=3):\n",
        "    \"\"\"\n",
        "    Prints a string with # box for emphasis.\n",
        "\n",
        "    Example:\n",
        "    ############################\n",
        "    #                          #\n",
        "    #  Applying ROME to model  #\n",
        "    #                          #\n",
        "    ############################\n",
        "    \"\"\"\n",
        "\n",
        "    n = len(x)\n",
        "    print()\n",
        "    print(\"\".join([\"#\" for _ in range(n + 2 * pad)]))\n",
        "    print(\"#\" + \"\".join([\" \" for _ in range(n + 2 * (pad - 1))]) + \"#\")\n",
        "    print(\n",
        "        \"#\"\n",
        "        + \"\".join([\" \" for _ in range(pad - 1)])\n",
        "        + x\n",
        "        + \"\".join([\" \" for _ in range(pad - 1)])\n",
        "        + \"#\"\n",
        "    )\n",
        "    print(\"#\" + \"\".join([\" \" for _ in range(n + 2 * (pad - 1))]) + \"#\")\n",
        "    print(\"\".join([\"#\" for _ in range(n + 2 * pad)]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring(\n",
        "    generation_prompts: List[str],\n",
        "    predict: List[str],\n",
        "    ans: List[Union[str, List[str]]]\n",
        "):\n",
        "    \"\"\"\n",
        "    Scoring function used in this homework.\n",
        "    Here we use accuracy as the simple and direct benchmark,\n",
        "    instead of comparing the probability.\n",
        "    \"\"\"\n",
        "    prompt_count = 0\n",
        "    correct_count = 0\n",
        "    for i in range(len(generation_prompts)):\n",
        "        prompt_count += 1\n",
        "        if isinstance(ans[i], str):\n",
        "            ans[i] = [ans[i]]\n",
        "        generation_prompt = generation_prompts[i].replace(\"'\", \"\").replace('\"', '').replace('.', '').replace(',', '').replace(':', '')\n",
        "        predict_prompt = predict[i].replace(\"'\", \"\").replace('\"', '').replace('.', '').replace(',', '').replace(':', '')\n",
        "        for cand in ans[i]:\n",
        "            if predict_prompt.startswith(f\"{generation_prompt} {cand}\"):\n",
        "                correct_count += 1\n",
        "                break\n",
        "    return correct_count / prompt_count"
      ],
      "metadata": {
        "id": "FpKB7WXLhbvv"
      },
      "id": "FpKB7WXLhbvv",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning Function\n",
        "This code is for the fine-tuning method."
      ],
      "metadata": {
        "id": "alMBF3wOSNXI"
      },
      "id": "alMBF3wOSNXI"
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FTHyperParams:\n",
        "    # Method\n",
        "    layers: List[int]\n",
        "    num_steps: int\n",
        "    lr: float\n",
        "    weight_decay: float\n",
        "    kl_factor: float\n",
        "    norm_constraint: float\n",
        "\n",
        "    # Module templates\n",
        "    rewrite_module_tmp: str\n",
        "    layer_module_tmp: str\n",
        "    mlp_module_tmp: str\n",
        "    attn_module_tmp: str\n",
        "    ln_f_module: str\n",
        "    lm_head_module: str\n",
        "\n",
        "    # Defaults\n",
        "    batch_size: int = 64\n",
        "    wd_power_law: tuple = None  # Scale weight decay by number of edits"
      ],
      "metadata": {
        "id": "6ua45WJWSll6"
      },
      "id": "6ua45WJWSll6",
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_hparam = {\n",
        "    \"layers\": [\n",
        "        0\n",
        "    ],\n",
        "    \"num_steps\": 25,\n",
        "    \"lr\": 5e-4,\n",
        "    \"weight_decay\": 0,\n",
        "    \"kl_factor\": 0,\n",
        "    \"norm_constraint\": 5e-4,\n",
        "    \"rewrite_module_tmp\": \"transformer.h.{}.mlp.c_proj\",\n",
        "    \"layer_module_tmp\": \"transformer.h.{}\",\n",
        "    \"mlp_module_tmp\": \"transformer.h.{}.mlp\",\n",
        "    \"attn_module_tmp\": \"transformer.h.{}.attn\",\n",
        "    \"ln_f_module\": \"transformer.ln_f\",\n",
        "    \"lm_head_module\": \"transformer.wte\"\n",
        "}"
      ],
      "metadata": {
        "id": "fodZFGM8Sp9N"
      },
      "id": "fodZFGM8Sp9N",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_ft_to_model(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    requests: List[Dict],\n",
        "    hparams: FTHyperParams,\n",
        "    copy=False,\n",
        "    return_orig_weights=False,\n",
        "    **kwargs: Any,\n",
        ") -> Tuple[AutoModelForCausalLM, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Returns a model with the desired changes.\n",
        "    :param copy: If true, will preserve the original model while creating a new one to edit.\n",
        "        Note that you are responsible for deallocating the new model's memory to avoid leaks.\n",
        "    :return: (1) the updated model, (2) the weights that changed\n",
        "    \"\"\"\n",
        "\n",
        "    weights_copy = {}\n",
        "    if copy:\n",
        "        model = deepcopy(model)\n",
        "\n",
        "    deltas = execute_ft(model, tok, requests, hparams)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for w_name, upd_matrix in deltas.items():\n",
        "            w = get_parameter(model, w_name)\n",
        "            if return_orig_weights and w_name not in weights_copy:\n",
        "                weights_copy[w_name] = w.detach().clone()\n",
        "\n",
        "            w[...] += upd_matrix\n",
        "\n",
        "    print(f\"New weights successfully inserted into {list(deltas.keys())}\")\n",
        "\n",
        "    return model, weights_copy\n",
        "\n",
        "\n",
        "def execute_ft(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    requests: List[Dict],\n",
        "    hparams: FTHyperParams,\n",
        "    **kwargs: Any,\n",
        ") -> Dict[str, Tuple[torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Executes the FT update algorithm for the specified update at the specified layer\n",
        "    Invariant: model at beginning of function == model at end of function\n",
        "    \"\"\"\n",
        "\n",
        "    # Update target and print info\n",
        "    requests = deepcopy(requests)\n",
        "    for request in requests:\n",
        "        if request[\"target_new\"][\"str\"][0] != \" \":\n",
        "            # Space required for correct tokenization\n",
        "            request[\"target_new\"][\"str\"] = \" \" + request[\"target_new\"][\"str\"]\n",
        "        print(\n",
        "            f\"Executing FT algo for: \"\n",
        "            f\"[{request['prompt'].format(request['subject'])}] -> [{request['target_new']['str']}]\"\n",
        "        )\n",
        "\n",
        "    # Retrieve weights that user desires to change\n",
        "    weights = {\n",
        "        n: p\n",
        "        for n, p in model.named_parameters()\n",
        "        for layer in hparams.layers\n",
        "        if hparams.rewrite_module_tmp.format(layer) in n\n",
        "    }\n",
        "    # Save old weights for future restoration\n",
        "    weights_copy = {k: v.detach().clone() for k, v in weights.items()}\n",
        "    print(f\"Weights to be updated: {list(weights.keys())}\")\n",
        "\n",
        "    # Define inputs\n",
        "    texts = [r[\"prompt\"].format(r[\"subject\"]) for r in requests]\n",
        "    targets = [r[\"target_new\"][\"str\"] for r in requests]\n",
        "\n",
        "    # Configure optimizer / gradients\n",
        "    wd = (\n",
        "        hparams.weight_decay\n",
        "        if not isinstance(hparams.wd_power_law, tuple)\n",
        "        else (len(requests) ** hparams.wd_power_law[0])\n",
        "        * np.exp(hparams.wd_power_law[1])\n",
        "    )\n",
        "    print(f\"Using weight decay of {wd} for {len(requests)} edits\")\n",
        "    opt = torch.optim.Adam(\n",
        "        [v for _, v in weights.items()],\n",
        "        lr=hparams.lr,\n",
        "        weight_decay=wd,\n",
        "    )\n",
        "    for name, w in model.named_parameters():\n",
        "        w.requires_grad = name in weights\n",
        "\n",
        "    # Update loop: intervene at layers simultaneously\n",
        "    loss_meter = AverageMeter()\n",
        "    for it in range(hparams.num_steps):\n",
        "        print(20 * \"=\")\n",
        "        print(f\"Epoch: {it}\")\n",
        "        print(20 * \"=\")\n",
        "        loss_meter.reset()\n",
        "\n",
        "        for txt, tgt in zip(\n",
        "            chunks(texts, hparams.batch_size), chunks(targets, hparams.batch_size)\n",
        "        ):\n",
        "            inputs = tok(txt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
        "            target_ids = tok(tgt, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(\n",
        "                \"cuda\"\n",
        "            )\n",
        "            last_token_inds = inputs[\"attention_mask\"].sum(dim=1) - 1\n",
        "            loss_mask = target_ids != tok.unk_token_id\n",
        "\n",
        "            opt.zero_grad()\n",
        "            bs = inputs[\"input_ids\"].shape[0]\n",
        "            probs = torch.nn.functional.log_softmax(\n",
        "                model(**inputs).logits[torch.arange(bs), last_token_inds], dim=-1\n",
        "            )\n",
        "            loss = -(torch.gather(probs, 1, target_ids) * loss_mask).sum(\n",
        "                1\n",
        "            ) / loss_mask.sum(1)\n",
        "            loss = loss.mean()\n",
        "            print(f\"Batch loss {loss.item()}\")\n",
        "            loss_meter.update(loss.item(), n=bs)\n",
        "\n",
        "            if loss.item() >= 1e-2:\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            if type(hparams.norm_constraint) is float:\n",
        "                eps = hparams.norm_constraint\n",
        "                with torch.no_grad():\n",
        "                    for k, v in weights.items():\n",
        "                        v[...] = torch.clamp(\n",
        "                            v, min=weights_copy[k] - eps, max=weights_copy[k] + eps\n",
        "                        )\n",
        "\n",
        "        print(f\"Total loss {loss_meter.avg}\")\n",
        "\n",
        "        if loss_meter.avg < 1e-2:\n",
        "            break\n",
        "\n",
        "    deltas = {k: (weights[k] - weights_copy[k]).detach() for k in weights}\n",
        "\n",
        "    # Restore state of original model\n",
        "    with torch.no_grad():\n",
        "        for k, v in weights.items():\n",
        "            v[...] = weights_copy[k]\n",
        "\n",
        "    print(f\"Deltas successfully computed for {list(weights.keys())}\")\n",
        "\n",
        "    return deltas\n",
        "\n",
        "\n",
        "def chunks(arr, n):\n",
        "    \"\"\"Yield successive n-sized chunks from arr.\"\"\"\n",
        "    chunk = []\n",
        "    for a in arr:\n",
        "        chunk.append(a)\n",
        "        if len(chunk) == n:\n",
        "            yield chunk\n",
        "            chunk = []\n",
        "    if len(chunk) > 0:\n",
        "        yield chunk\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "Ve04AemMSM9-"
      },
      "id": "Ve04AemMSM9-",
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Dc8kHiSwVeBd",
      "metadata": {
        "id": "Dc8kHiSwVeBd"
      },
      "source": [
        "### ROME Function\n",
        "This code is for the ROME method. **MODIFY THE CODE IN THE MAIN FUNCTION!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HyperParams"
      ],
      "metadata": {
        "id": "XIDt0sHoIhxH"
      },
      "id": "XIDt0sHoIhxH"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "Oi57T8O3Lsf6",
      "metadata": {
        "id": "Oi57T8O3Lsf6"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ROMEHyperParams:\n",
        "    # Method\n",
        "    layers: List[int]\n",
        "    fact_token: str\n",
        "    v_num_grad_steps: int\n",
        "    v_lr: float\n",
        "    v_loss_layer: int\n",
        "    v_weight_decay: float\n",
        "    clamp_norm_factor: float\n",
        "    kl_factor: float\n",
        "    mom2_adjustment: bool\n",
        "    context_template_length_params: List[List[int]]\n",
        "\n",
        "    # Module templates\n",
        "    rewrite_module_tmp: str\n",
        "    layer_module_tmp: str\n",
        "    mlp_module_tmp: str\n",
        "    attn_module_tmp: str\n",
        "    ln_f_module: str\n",
        "    lm_head_module: str\n",
        "\n",
        "    # Statistics\n",
        "    mom2_dataset: str\n",
        "    mom2_n_samples: int\n",
        "    mom2_dtype: str\n",
        "\n",
        "    @classmethod\n",
        "    def from_json(cls, fpath):\n",
        "        with open(fpath, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        return cls(**data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "_AzBoNII1lw_",
      "metadata": {
        "id": "_AzBoNII1lw_"
      },
      "outputs": [],
      "source": [
        "rome_hparam = {\n",
        "    \"layers\": [\n",
        "        17\n",
        "    ],\n",
        "    \"fact_token\": \"subject_last\",\n",
        "    \"v_num_grad_steps\": 20,\n",
        "    \"v_lr\": 5e-1,\n",
        "    \"v_loss_layer\": 47,\n",
        "    \"v_weight_decay\": 0.5,\n",
        "    \"clamp_norm_factor\": 4,\n",
        "    \"kl_factor\": 0.0625,\n",
        "    \"mom2_adjustment\": True,\n",
        "    \"context_template_length_params\": [[5, 10], [10, 10]],\n",
        "    \"rewrite_module_tmp\": \"transformer.h.{}.mlp.c_proj\",\n",
        "    \"layer_module_tmp\": \"transformer.h.{}\",\n",
        "    \"mlp_module_tmp\": \"transformer.h.{}.mlp\",\n",
        "    \"attn_module_tmp\": \"transformer.h.{}.attn\",\n",
        "    \"ln_f_module\": \"transformer.ln_f\",\n",
        "    \"lm_head_module\": \"transformer.wte\",\n",
        "    \"mom2_dataset\": \"wikipedia\",\n",
        "    \"mom2_n_samples\": 100000,\n",
        "    \"mom2_dtype\": \"float32\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### compute_u and compute_v"
      ],
      "metadata": {
        "id": "PxEuFVVTIYbx"
      },
      "id": "PxEuFVVTIYbx"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_v(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    request: Dict,\n",
        "    hparams: ROMEHyperParams,\n",
        "    layer: int,\n",
        "    left_vector: torch.Tensor,\n",
        "    context_templates: List[str],\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the value (right) vector for the rank-1 update.\n",
        "    Runs a simple optimization procedure.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Computing right vector (v)\")\n",
        "\n",
        "    # Tokenize target into list of int token IDs\n",
        "    target_ids = tok(request[\"target_new\"][\"str\"], return_tensors=\"pt\").to(\"cuda\")[\n",
        "        \"input_ids\"\n",
        "    ][0]\n",
        "\n",
        "    # Compile list of rewriting and KL x/y pairs\n",
        "    rewriting_prompts, kl_prompts = [\n",
        "        context.format(request[\"prompt\"]) + tok.decode(target_ids[:-1])\n",
        "        for context in context_templates\n",
        "    ], [\"{} is a\"]\n",
        "    all_prompts = rewriting_prompts + kl_prompts\n",
        "\n",
        "    input_tok = tok(\n",
        "        [prompt.format(request[\"subject\"]) for prompt in all_prompts],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Compute rewriting targets\n",
        "    rewriting_targets = torch.tensor(-100, device=\"cuda\").repeat(\n",
        "        len(rewriting_prompts), *input_tok[\"input_ids\"].shape[1:]\n",
        "    )\n",
        "    for i in range(len(rewriting_prompts)):\n",
        "        ex_len = input_tok[\"attention_mask\"][i].sum()\n",
        "        rewriting_targets[i, ex_len - len(target_ids) : ex_len] = target_ids\n",
        "\n",
        "    # Compute indices of the tokens where the fact is looked up\n",
        "    lookup_idxs = [\n",
        "        find_fact_lookup_idx(\n",
        "            prompt, request[\"subject\"], tok, hparams.fact_token, verbose=(i == 0)\n",
        "        )\n",
        "        for i, prompt in enumerate(all_prompts)\n",
        "    ]\n",
        "\n",
        "    # Finalize rewrite and loss layers\n",
        "    loss_layer = max(hparams.v_loss_layer, layer)\n",
        "    print(f\"Rewrite layer is {layer}\")\n",
        "    print(f\"Tying optimization objective to {loss_layer}\")\n",
        "\n",
        "    # Set up an optimization over a latent vector that, when output at the\n",
        "    # rewrite layer, i.e. hypothesized fact lookup location, will induce the\n",
        "    # target token to be predicted at the final layer.\n",
        "    delta = torch.zeros((model.config.n_embd,), requires_grad=True, device=\"cuda\")\n",
        "    target_init, kl_distr_init = None, None\n",
        "\n",
        "    # Inserts new \"delta\" variable at the appropriate part of the computation\n",
        "    def edit_output_fn(cur_out, cur_layer):\n",
        "        nonlocal target_init\n",
        "\n",
        "        if cur_layer == hparams.mlp_module_tmp.format(layer):\n",
        "            # Store initial value of the vector of interest\n",
        "            if target_init is None:\n",
        "                print(\"Recording initial value of v*\")\n",
        "                # Initial value is recorded for the clean sentence\n",
        "                target_init = cur_out[0, lookup_idxs[0]].detach().clone()\n",
        "\n",
        "            for i, idx in enumerate(lookup_idxs):\n",
        "                cur_out[i, idx, :] += delta\n",
        "\n",
        "        return cur_out\n",
        "\n",
        "    # Optimizer\n",
        "    opt = torch.optim.Adam([delta], lr=hparams.v_lr)\n",
        "    nethook.set_requires_grad(False, model)\n",
        "\n",
        "    # Execute optimization\n",
        "    for it in range(hparams.v_num_grad_steps):\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        with nethook.TraceDict(\n",
        "            module=model,\n",
        "            layers=[\n",
        "                hparams.layer_module_tmp.format(loss_layer),\n",
        "                hparams.mlp_module_tmp.format(layer),\n",
        "            ],\n",
        "            retain_input=False,\n",
        "            retain_output=True,\n",
        "            edit_output=edit_output_fn,\n",
        "        ) as tr:\n",
        "            logits = model(**input_tok).logits\n",
        "\n",
        "            # Compute distribution for KL divergence\n",
        "            kl_logits = torch.stack(\n",
        "                [\n",
        "                    logits[i - len(kl_prompts), idx, :]\n",
        "                    for i, idx in enumerate(lookup_idxs[-len(kl_prompts) :])\n",
        "                ],\n",
        "                dim=0,\n",
        "            )\n",
        "            kl_log_probs = torch.nn.functional.log_softmax(kl_logits, dim=1)\n",
        "            if kl_distr_init is None:\n",
        "                kl_distr_init = kl_log_probs.detach().clone()\n",
        "\n",
        "        # Compute loss on rewriting targets\n",
        "        log_probs = torch.log_softmax(logits, dim=2)\n",
        "\n",
        "        loss = torch.gather(\n",
        "            log_probs,\n",
        "            2,\n",
        "            torch.where(rewriting_targets != -100, rewriting_targets, 0).unsqueeze(2),\n",
        "        ).squeeze(2)\n",
        "        mask = (rewriting_targets != -100).float()\n",
        "\n",
        "        # Aggregate total losses\n",
        "        nll_loss_each = -(loss * mask).sum(1) / target_ids.size(0)\n",
        "        nll_loss = nll_loss_each.mean()\n",
        "        kl_loss = hparams.kl_factor * torch.nn.functional.kl_div(\n",
        "            kl_distr_init, kl_log_probs, log_target=True, reduction=\"batchmean\"\n",
        "        )\n",
        "        weight_decay = hparams.v_weight_decay * (\n",
        "            torch.norm(delta) / torch.norm(target_init) ** 2\n",
        "        )\n",
        "        # weight_decay = hparams.v_weight_decay * torch.norm(delta) ** 2\n",
        "        loss = nll_loss + kl_loss + weight_decay\n",
        "        print(\n",
        "            f\"loss {np.round(loss.item(), 3)} = {np.round(nll_loss.item(), 3)} + {np.round(kl_loss.item(), 3)} + {np.round(weight_decay.item(), 3)} \"\n",
        "            f\"avg prob of [{request['target_new']['str']}] \"\n",
        "            f\"{torch.exp(-nll_loss_each).mean().item()}\"\n",
        "        )\n",
        "        if loss < 5e-2:\n",
        "            break\n",
        "\n",
        "        if it == hparams.v_num_grad_steps - 1:\n",
        "            break\n",
        "\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # Project within L2 ball\n",
        "        max_norm = hparams.clamp_norm_factor * target_init.norm()\n",
        "        if delta.norm() > max_norm:\n",
        "            with torch.no_grad():\n",
        "                delta[...] = delta * max_norm / delta.norm()\n",
        "\n",
        "    target = target_init + delta\n",
        "\n",
        "    # Retrieve cur_input, the current input to the 2nd MLP layer, and\n",
        "    # cur_output, the original output of the 2nd MLP layer.\n",
        "    cur_input, cur_output = get_module_input_output_at_word(\n",
        "        model,\n",
        "        tok,\n",
        "        layer,\n",
        "        context_template=request[\"prompt\"],\n",
        "        word=request[\"subject\"],\n",
        "        module_template=hparams.rewrite_module_tmp,\n",
        "        fact_token_strategy=hparams.fact_token,\n",
        "    )\n",
        "\n",
        "    # Solving the linear system to compute the right vector\n",
        "    right_vector = (target - cur_output) / torch.dot(cur_input, left_vector)\n",
        "    print(f\"Delta norm: {(target - cur_output).norm().item()}\")\n",
        "    print(\n",
        "        f\"Change in target norm: {target_init.norm().item()} to {target.norm().item()} => {(target.norm() - target_init.norm()).item()}\"\n",
        "    )\n",
        "    print(f\"Division Factor: {torch.dot(cur_input, left_vector).item()}\")\n",
        "    print(f\"Right vector norm: {right_vector.norm()}\")\n",
        "\n",
        "    return right_vector\n",
        "\n",
        "\n",
        "def get_module_input_output_at_word(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    layer: int,\n",
        "    context_template: str,\n",
        "    word: str,\n",
        "    module_template: str,\n",
        "    fact_token_strategy: str,\n",
        ") -> Tuple[torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Retrieves detached representations for a word at the input and\n",
        "    output of a particular layer module.\n",
        "    \"\"\"\n",
        "\n",
        "    word_repr_args = dict(\n",
        "        model=model,\n",
        "        tok=tok,\n",
        "        layer=layer,\n",
        "        module_template=module_template,\n",
        "    )\n",
        "    if \"subject_\" in fact_token_strategy and fact_token_strategy.index(\"subject_\") == 0:\n",
        "        subtoken = fact_token_strategy[len(\"subject_\") :]\n",
        "        l_input, l_output = repr_tools.get_reprs_at_word_tokens(\n",
        "            track=\"both\",\n",
        "            subtoken=subtoken,\n",
        "            context_templates=[context_template],\n",
        "            words=[word],\n",
        "            **word_repr_args,\n",
        "        )\n",
        "    elif fact_token_strategy == \"last\":\n",
        "        l_input, l_output = repr_tools.get_reprs_at_idxs(\n",
        "            track=\"both\",\n",
        "            contexts=[context_template.format(word)],\n",
        "            idxs=[[-1]],\n",
        "            **word_repr_args,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"fact_token={fact_token_strategy} not recognized\")\n",
        "\n",
        "    l_input, l_output = l_input[0], l_output[0]\n",
        "    return l_input.detach(), l_output.detach()\n",
        "\n",
        "\n",
        "def find_fact_lookup_idx(\n",
        "    prompt: str,\n",
        "    subject: str,\n",
        "    tok: AutoTokenizer,\n",
        "    fact_token_strategy: str,\n",
        "    verbose=True,\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Computes hypothesized fact lookup index given a sentence and subject.\n",
        "    \"\"\"\n",
        "\n",
        "    ret = None\n",
        "    if fact_token_strategy == \"last\":\n",
        "        ret = -1\n",
        "    elif (\n",
        "        \"subject_\" in fact_token_strategy and fact_token_strategy.index(\"subject_\") == 0\n",
        "    ):\n",
        "        ret = repr_tools.get_words_idxs_in_templates(\n",
        "            tok=tok,\n",
        "            context_templates=[prompt],\n",
        "            words=[subject],\n",
        "            subtoken=fact_token_strategy[len(\"subject_\") :],\n",
        "        )[0][0]\n",
        "    else:\n",
        "        raise ValueError(f\"fact_token={fact_token_strategy} not recognized\")\n",
        "\n",
        "    sentence = prompt.format(subject)\n",
        "    if verbose:\n",
        "        print(\n",
        "            f\"Lookup index found: {ret} | Sentence: {sentence} | Token:\",\n",
        "            tok.decode(tok(sentence)[\"input_ids\"][ret]),\n",
        "        )\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "iVOvvXoeHaeE"
      },
      "id": "iVOvvXoeHaeE",
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache variables\n",
        "inv_mom2_cache = {}\n",
        "\n",
        "\n",
        "def get_inv_cov(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    layer_name: str,\n",
        "    mom2_dataset: str,\n",
        "    mom2_n_samples: str,\n",
        "    mom2_dtype: str,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Retrieves covariance statistics, then computes the algebraic inverse.\n",
        "    Caches result for future use.\n",
        "    \"\"\"\n",
        "\n",
        "    global inv_mom2_cache\n",
        "\n",
        "    model_name = model.config._name_or_path.replace(\"/\", \"_\")\n",
        "    key = (model_name, layer_name)\n",
        "\n",
        "    if key not in inv_mom2_cache:\n",
        "        print(\n",
        "            f\"Retrieving inverse covariance statistics for {model_name} @ {layer_name}. \"\n",
        "            f\"The result will be cached to avoid repetitive computation.\"\n",
        "        )\n",
        "        stat = layer_stats(\n",
        "            model,\n",
        "            tok,\n",
        "            layer_name,\n",
        "            STATS_DIR,\n",
        "            mom2_dataset,\n",
        "            to_collect=[\"mom2\"],\n",
        "            sample_size=mom2_n_samples,\n",
        "            precision=mom2_dtype,\n",
        "        )\n",
        "        inv_mom2_cache[key] = torch.inverse(\n",
        "            stat.mom2.moment().to(\"cuda\")\n",
        "        ).float()  # Cast back to float32\n",
        "\n",
        "    return inv_mom2_cache[key]\n",
        "\n",
        "\n",
        "def compute_u(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    request: Dict,\n",
        "    hparams: ROMEHyperParams,\n",
        "    layer: int,\n",
        "    context_templates: List[str],\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the left vector used in constructing the rank-1 update matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Computing left vector (u)...\")\n",
        "\n",
        "    # Compute projection token\n",
        "    word_repr_args = dict(\n",
        "        model=model,\n",
        "        tok=tok,\n",
        "        layer=layer,\n",
        "        module_template=hparams.rewrite_module_tmp,\n",
        "        track=\"in\",\n",
        "    )\n",
        "    if \"subject_\" in hparams.fact_token and hparams.fact_token.index(\"subject_\") == 0:\n",
        "        word = request[\"subject\"]\n",
        "        print(f\"Selected u projection object {word}\")\n",
        "        cur_repr = repr_tools.get_reprs_at_word_tokens(\n",
        "            context_templates=[\n",
        "                templ.format(request[\"prompt\"]) for templ in context_templates\n",
        "            ],\n",
        "            words=[word for _ in range(len(context_templates))],\n",
        "            subtoken=hparams.fact_token[len(\"subject_\") :],\n",
        "            **word_repr_args,\n",
        "        ).mean(0)\n",
        "    elif hparams.fact_token == \"last\":\n",
        "        # Heuristic to choose last word. Not a huge deal if there's a minor\n",
        "        # edge case (e.g. multi-token word) because the function below will\n",
        "        # take the last token.\n",
        "        cur_repr = repr_tools.get_reprs_at_idxs(\n",
        "            contexts=[\n",
        "                templ.format(request[\"prompt\"].format(request[\"subject\"]))\n",
        "                for templ in context_templates\n",
        "            ],\n",
        "            idxs=[[-1] for _ in range(len(context_templates))],\n",
        "            **word_repr_args,\n",
        "        ).mean(0)\n",
        "        print(\"Selected u projection token with last token\")\n",
        "    else:\n",
        "        raise ValueError(f\"fact_token={hparams.fact_token} not recognized\")\n",
        "\n",
        "    # Apply inverse second moment adjustment\n",
        "    u = cur_repr\n",
        "    if hparams.mom2_adjustment:\n",
        "        u = get_inv_cov(\n",
        "            model,\n",
        "            tok,\n",
        "            hparams.rewrite_module_tmp.format(layer),\n",
        "            hparams.mom2_dataset,\n",
        "            hparams.mom2_n_samples,\n",
        "            hparams.mom2_dtype,\n",
        "        ) @ u.unsqueeze(1)\n",
        "        u = u.squeeze()\n",
        "\n",
        "    return u / u.norm()"
      ],
      "metadata": {
        "id": "04cO1H5ZHUNH"
      },
      "id": "04cO1H5ZHUNH",
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Function"
      ],
      "metadata": {
        "id": "a1CzSH2NImPF"
      },
      "id": "a1CzSH2NImPF"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "d2_Kq02bVsz9",
      "metadata": {
        "id": "d2_Kq02bVsz9"
      },
      "outputs": [],
      "source": [
        "CONTEXT_TEMPLATES_CACHE = None\n",
        "\n",
        "\n",
        "def apply_rome_to_model(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    requests: List[Dict],\n",
        "    hparams: ROMEHyperParams,\n",
        "    copy=False,\n",
        "    return_orig_weights=False,\n",
        ") -> Tuple[AutoModelForCausalLM, List[str]]:\n",
        "    \"\"\"\n",
        "    This function call execute_rome() and combine the results into a single matrix.\n",
        "    :param copy: If true, will preserve the original model while creating a new one to edit.\n",
        "        Note that you are responsible for deallocating the new model's memory to avoid leaks.\n",
        "    \"\"\"\n",
        "\n",
        "    if copy:\n",
        "        model = deepcopy(model)\n",
        "\n",
        "    weights_copy = {}\n",
        "\n",
        "    for i, request in enumerate(requests):\n",
        "        deltas = execute_rome(model, tok, request, hparams)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for w_name, (delta_u, delta_v) in deltas.items():\n",
        "                ###### TODO: Complete the code below ######\n",
        "                \"\"\"\n",
        "                Hint: Take a look at execute_rome(), compute_u() and compute_v()\n",
        "                The answer is simply the outer product of two vectors\n",
        "                Note that the weight of GPT2-XL is transposed\n",
        "                \"\"\"\n",
        "                delta_u = delta_u.view(-1)\n",
        "                delta_v = delta_v.view(-1)\n",
        "                upd_matrix = torch.outer(delta_u, delta_v)\n",
        "                upd_matrix = torch.transpose(upd_matrix, 0, 1)\n",
        "                w = get_parameter(model, w_name)\n",
        "                upd_matrix = upd_matrix_match_shape(upd_matrix, w.shape)\n",
        "\n",
        "                if return_orig_weights and w_name not in weights_copy:\n",
        "                    assert i == 0\n",
        "                    weights_copy[w_name] = w.detach().clone()\n",
        "\n",
        "                w[...] += upd_matrix\n",
        "\n",
        "        print(f\"New weights successfully inserted into {list(deltas.keys())}\")\n",
        "\n",
        "    return model, weights_copy\n",
        "\n",
        "\n",
        "def execute_rome(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    request: Dict,\n",
        "    hparams: ROMEHyperParams,\n",
        ") -> Dict[str, Tuple[torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Executes the ROME update algorithm for the specified update at the specified layer\n",
        "    Invariant: model at beginning of function == model at end of function\n",
        "    \"\"\"\n",
        "\n",
        "    # Update target and print info\n",
        "    request = deepcopy(request)\n",
        "    if request[\"target_new\"][\"str\"][0] != \" \":\n",
        "        # Space required for correct tokenization\n",
        "        request[\"target_new\"][\"str\"] = \" \" + request[\"target_new\"][\"str\"]\n",
        "    print(\n",
        "        f\"Executing ROME algorithm for the update: \"\n",
        "        f\"[{request['prompt'].format(request['subject'])}] -> [{request['target_new']['str']}]\"\n",
        "    )\n",
        "\n",
        "    # Retrieve weights that user desires to change\n",
        "    weights = {\n",
        "        f\"{hparams.rewrite_module_tmp.format(layer)}.weight\": get_parameter(\n",
        "            model, f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
        "        )\n",
        "        for layer in hparams.layers\n",
        "    }\n",
        "    # Save old weights for future restoration\n",
        "    weights_copy = {k: v.detach().clone() for k, v in weights.items()}\n",
        "\n",
        "    # Update loop: sequentially intervene at each specified layer\n",
        "    deltas = {}\n",
        "    for layer in sorted(hparams.layers):\n",
        "        # Compute rank-1 update matrix\n",
        "        left_vector: torch.Tensor = compute_u(\n",
        "            model,\n",
        "            tok,\n",
        "            request,\n",
        "            hparams,\n",
        "            layer,\n",
        "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
        "        )\n",
        "        print(\"Left vector shape:\", left_vector.shape)\n",
        "        right_vector: torch.Tensor = compute_v(\n",
        "            model,\n",
        "            tok,\n",
        "            request,\n",
        "            hparams,\n",
        "            layer,\n",
        "            left_vector,\n",
        "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
        "        )\n",
        "        print(\"Right vector shape:\", right_vector.shape)\n",
        "\n",
        "        left_vector = left_vector.unsqueeze(1)\n",
        "        right_vector = right_vector.unsqueeze(0)\n",
        "        weight_name = f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
        "        deltas[weight_name] = (\n",
        "            left_vector.detach(),\n",
        "            right_vector.detach(),\n",
        "        )\n",
        "\n",
        "    print(f\"Deltas successfully computed for {list(weights.keys())}\")\n",
        "\n",
        "    return deltas\n",
        "\n",
        "\n",
        "def upd_matrix_match_shape(matrix: torch.Tensor, shape: torch.Size) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    GPT-2 and GPT-J have transposed weight representations.\n",
        "    Returns a matrix that matches the desired shape, else raises a ValueError\n",
        "    \"\"\"\n",
        "\n",
        "    if matrix.shape == shape:\n",
        "        return matrix\n",
        "    elif matrix.T.shape == shape:\n",
        "        return matrix.T\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Update matrix computed by ROME does not match original weight shape. \"\n",
        "            \"Check for bugs in the code?\"\n",
        "        )\n",
        "\n",
        "\n",
        "def get_context_templates(model, tok, length_params):\n",
        "    global CONTEXT_TEMPLATES_CACHE\n",
        "\n",
        "    if CONTEXT_TEMPLATES_CACHE is None:\n",
        "        CONTEXT_TEMPLATES_CACHE = [\"{}\"] + [\n",
        "            x + \". {}\"\n",
        "            for x in sum(\n",
        "                (\n",
        "                    generate(\n",
        "                        model,\n",
        "                        tok,\n",
        "                        [\"<|endoftext|>\"],\n",
        "                        n_gen_per_prompt=n_gen,\n",
        "                        max_out_len=length,\n",
        "                    )\n",
        "                    for length, n_gen in length_params\n",
        "                ),\n",
        "                [],\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        print(f\"Cached context templates {CONTEXT_TEMPLATES_CACHE}\")\n",
        "\n",
        "    return CONTEXT_TEMPLATES_CACHE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### MEMIT function"
      ],
      "metadata": {
        "id": "Yg7anUcW-QmD"
      },
      "id": "Yg7anUcW-QmD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HyperParams"
      ],
      "metadata": {
        "id": "LtWyJFMs-gAC"
      },
      "id": "LtWyJFMs-gAC"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Literal\n",
        "@dataclass\n",
        "class MEMITHyperParams(HyperParams):\n",
        "    # Method\n",
        "    layers: List[int]\n",
        "    layer_selection: Literal[\"all\", \"random\"]\n",
        "    fact_token: Literal[\n",
        "        \"last\", \"subject_first\", \"subject_last\", \"subject_first_after_last\"\n",
        "    ]\n",
        "    v_num_grad_steps: int\n",
        "    v_lr: float\n",
        "    v_loss_layer: int\n",
        "    v_weight_decay: float\n",
        "    clamp_norm_factor: float\n",
        "    kl_factor: float\n",
        "    mom2_adjustment: bool\n",
        "    mom2_update_weight: float\n",
        "\n",
        "    # Module templates\n",
        "    rewrite_module_tmp: str\n",
        "    layer_module_tmp: str\n",
        "    mlp_module_tmp: str\n",
        "    attn_module_tmp: str\n",
        "    ln_f_module: str\n",
        "    lm_head_module: str\n",
        "\n",
        "    # Statistics\n",
        "    mom2_dataset: str\n",
        "    mom2_n_samples: int\n",
        "    mom2_dtype: str"
      ],
      "metadata": {
        "id": "5SQGh7LM-mPT"
      },
      "id": "5SQGh7LM-mPT",
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memit_hparam = {\n",
        "    \"layers\": [13, 14, 15, 16, 17],\n",
        "    \"clamp_norm_factor\": 0.75,\n",
        "    \"layer_selection\": \"all\",\n",
        "    \"fact_token\": \"subject_last\",\n",
        "    \"v_num_grad_steps\": 20,\n",
        "    \"v_lr\": 5e-1,\n",
        "    \"v_loss_layer\": 47,\n",
        "    \"v_weight_decay\": 0.5,\n",
        "    \"kl_factor\": 0.0625,\n",
        "    \"mom2_adjustment\": True,\n",
        "    \"mom2_update_weight\": 20000,\n",
        "    \"rewrite_module_tmp\": \"transformer.h.{}.mlp.c_proj\",\n",
        "    \"layer_module_tmp\": \"transformer.h.{}\",\n",
        "    \"mlp_module_tmp\": \"transformer.h.{}.mlp\",\n",
        "    \"attn_module_tmp\": \"transformer.h.{}.attn\",\n",
        "    \"ln_f_module\": \"transformer.ln_f\",\n",
        "    \"lm_head_module\": \"transformer.wte\",\n",
        "    \"mom2_dataset\": \"wikipedia\",\n",
        "    \"mom2_n_samples\": 100000,\n",
        "    \"mom2_dtype\": \"float32\"\n",
        "}"
      ],
      "metadata": {
        "id": "HrAeVCbq-yZu"
      },
      "id": "HrAeVCbq-yZu",
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utils"
      ],
      "metadata": {
        "id": "MRNTfNF0-0SA"
      },
      "id": "MRNTfNF0-0SA"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_z(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    request: Dict,\n",
        "    hparams: MEMITHyperParams,\n",
        "    layer: int,\n",
        "    context_templates: List[str],\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Computes the value (right) vector for the rank-1 update.\n",
        "    Runs a simple optimization procedure.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get model parameters\n",
        "    lm_w, ln_f = (\n",
        "        nethook.get_parameter(model, f\"{hparams.lm_head_module}.weight\").T,\n",
        "        nethook.get_module(model, hparams.ln_f_module),\n",
        "    )\n",
        "    try:\n",
        "        lm_b = nethook.get_parameter(model, f\"{hparams.lm_head_module}.bias\")\n",
        "    except LookupError as _:\n",
        "        lm_b = next(model.parameters()).new_zeros(model.config.vocab_size)\n",
        "\n",
        "    print(\"Computing right vector (v)\")\n",
        "\n",
        "    # Tokenize target into list of int token IDs\n",
        "    target_ids = tok(request[\"target_new\"][\"str\"], return_tensors=\"pt\").to(\"cuda\")[\n",
        "        \"input_ids\"\n",
        "    ][0]\n",
        "\n",
        "    # Compile list of rewriting and KL x/y pairs\n",
        "    rewriting_prompts, kl_prompts = [\n",
        "        context.format(request[\"prompt\"]) + tok.decode(target_ids[:-1])\n",
        "        for context_types in context_templates\n",
        "        for context in context_types\n",
        "    ], [\"{} is a\"]\n",
        "    all_prompts = rewriting_prompts + kl_prompts\n",
        "\n",
        "    input_tok = tok(\n",
        "        [prompt.format(request[\"subject\"]) for prompt in all_prompts],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Compute rewriting targets\n",
        "    rewriting_targets = torch.tensor(-100, device=\"cuda\").repeat(\n",
        "        len(rewriting_prompts), *input_tok[\"input_ids\"].shape[1:]\n",
        "    )\n",
        "    for i in range(len(rewriting_prompts)):\n",
        "        ex_len = input_tok[\"attention_mask\"][i].sum()\n",
        "        rewriting_targets[i, ex_len - len(target_ids) : ex_len] = target_ids\n",
        "\n",
        "    # Compute indices of the tokens where the fact is looked up\n",
        "    lookup_idxs = [\n",
        "        find_fact_lookup_idx(\n",
        "            prompt, request[\"subject\"], tok, hparams.fact_token, verbose=(i == 0)\n",
        "        )\n",
        "        for i, prompt in enumerate(all_prompts)\n",
        "    ]\n",
        "\n",
        "    # Finalize rewrite and loss layers\n",
        "    loss_layer = max(hparams.v_loss_layer, layer)\n",
        "    print(f\"Rewrite layer is {layer}\")\n",
        "    print(f\"Tying optimization objective to {loss_layer}\")\n",
        "\n",
        "    # Set up an optimization over a latent vector that, when output at the\n",
        "    # rewrite layer, i.e. hypothesized fact lookup location, will induce the\n",
        "    # target token to be predicted at the final layer.\n",
        "    delta = torch.zeros((model.config.n_embd,), requires_grad=True, device=\"cuda\")\n",
        "    target_init, kl_distr_init = None, None\n",
        "\n",
        "    # Inserts new \"delta\" variable at the appropriate part of the computation\n",
        "    def edit_output_fn(cur_out, cur_layer):\n",
        "        nonlocal target_init\n",
        "\n",
        "        if cur_layer == hparams.layer_module_tmp.format(layer):\n",
        "            # Store initial value of the vector of interest\n",
        "            if target_init is None:\n",
        "                print(\"Recording initial value of v*\")\n",
        "                # Initial value is recorded for the clean sentence\n",
        "                target_init = cur_out[0][0, lookup_idxs[0]].detach().clone()\n",
        "\n",
        "            # Add intervened delta\n",
        "            for i, idx in enumerate(lookup_idxs):\n",
        "                cur_out[0][i, idx, :] += delta\n",
        "\n",
        "        return cur_out\n",
        "\n",
        "    # Optimizer\n",
        "    opt = torch.optim.Adam([delta], lr=hparams.v_lr)\n",
        "    nethook.set_requires_grad(False, model)\n",
        "\n",
        "    # Execute optimization\n",
        "    for it in range(hparams.v_num_grad_steps):\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        with nethook.TraceDict(\n",
        "            module=model,\n",
        "            layers=[\n",
        "                hparams.layer_module_tmp.format(loss_layer),\n",
        "                hparams.layer_module_tmp.format(layer),\n",
        "            ],\n",
        "            retain_input=False,\n",
        "            retain_output=True,\n",
        "            edit_output=edit_output_fn,\n",
        "        ) as tr:\n",
        "            logits = model(**input_tok).logits\n",
        "\n",
        "            # Compute distribution for KL divergence\n",
        "            kl_logits = torch.stack(\n",
        "                [\n",
        "                    logits[i - len(kl_prompts), idx, :]\n",
        "                    for i, idx in enumerate(lookup_idxs[-len(kl_prompts) :])\n",
        "                ],\n",
        "                dim=0,\n",
        "            )\n",
        "            kl_log_probs = torch.nn.functional.log_softmax(kl_logits, dim=1)\n",
        "            if kl_distr_init is None:\n",
        "                kl_distr_init = kl_log_probs.detach().clone()\n",
        "\n",
        "        # Compute loss on rewriting targets\n",
        "        full_repr = tr[hparams.layer_module_tmp.format(loss_layer)].output[0][\n",
        "            : len(rewriting_prompts)\n",
        "        ]\n",
        "        log_probs = torch.log_softmax(ln_f(full_repr) @ lm_w + lm_b, dim=2)\n",
        "        loss = torch.gather(\n",
        "            log_probs,\n",
        "            2,\n",
        "            torch.where(rewriting_targets != -100, rewriting_targets, 0).unsqueeze(2),\n",
        "        ).squeeze(2)\n",
        "        mask = (rewriting_targets != -100).float()\n",
        "\n",
        "        # Aggregate total losses\n",
        "        nll_loss_each = -(loss * mask).sum(1) / target_ids.size(0)\n",
        "        nll_loss = nll_loss_each.mean()\n",
        "        kl_loss = hparams.kl_factor * torch.nn.functional.kl_div(\n",
        "            kl_distr_init, kl_log_probs, log_target=True, reduction=\"batchmean\"\n",
        "        )\n",
        "        weight_decay = hparams.v_weight_decay * (\n",
        "            torch.norm(delta) / torch.norm(target_init) ** 2\n",
        "        )\n",
        "        # weight_decay = hparams.v_weight_decay * torch.norm(delta) ** 2\n",
        "        loss = nll_loss + kl_loss + weight_decay\n",
        "        print(\n",
        "            f\"loss {np.round(loss.item(), 3)} = {np.round(nll_loss.item(), 3)} + {np.round(kl_loss.item(), 3)} + {np.round(weight_decay.item(), 3)} \"\n",
        "            f\"avg prob of [{request['target_new']['str']}] \"\n",
        "            f\"{torch.exp(-nll_loss_each).mean().item()}\"\n",
        "        )\n",
        "        if loss < 5e-2:\n",
        "            break\n",
        "\n",
        "        if it == hparams.v_num_grad_steps - 1:\n",
        "            break\n",
        "\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # Project within L2 ball\n",
        "        max_norm = hparams.clamp_norm_factor * target_init.norm()\n",
        "        if delta.norm() > max_norm:\n",
        "            with torch.no_grad():\n",
        "                delta[...] = delta * max_norm / delta.norm()\n",
        "\n",
        "    target = target_init + delta\n",
        "    print(\n",
        "        f\"Init norm {target_init.norm()} | Delta norm {delta.norm()} | Target norm {target.norm()}\"\n",
        "    )\n",
        "\n",
        "    return target\n",
        "\n",
        "\n",
        "def get_module_input_output_at_words(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    layer: int,\n",
        "    context_templates: List[str],\n",
        "    words: List[str],\n",
        "    module_template: str,\n",
        "    fact_token_strategy: str,\n",
        ") -> Tuple[torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Retrieves detached representations for a word at the input and\n",
        "    output of a particular layer module.\n",
        "    \"\"\"\n",
        "\n",
        "    word_repr_args = dict(\n",
        "        model=model,\n",
        "        tok=tok,\n",
        "        layer=layer,\n",
        "        module_template=module_template,\n",
        "    )\n",
        "    if \"subject_\" in fact_token_strategy and fact_token_strategy.index(\"subject_\") == 0:\n",
        "        context_info = dict(\n",
        "            context_templates=context_templates,\n",
        "            words=words,\n",
        "        )\n",
        "        subtoken = fact_token_strategy[len(\"subject_\") :]\n",
        "        l_input, l_output = repr_tools.get_reprs_at_word_tokens(\n",
        "            track=\"both\", subtoken=subtoken, **context_info, **word_repr_args\n",
        "        )\n",
        "    elif fact_token_strategy == \"last\":\n",
        "        raise Exception(\"This is definitely bugged, fix it.\")\n",
        "        context_info = dict(\n",
        "            contexts=[\n",
        "                tmp[i].format(words[i]) for i, tmp in enumerate(context_templates)\n",
        "            ],\n",
        "            idxs=[000000],\n",
        "        )\n",
        "        l_input, l_output = repr_tools.get_reprs_at_idxs(\n",
        "            track=\"both\", **context_info, **word_repr_args\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"fact_token={fact_token_strategy} not recognized\")\n",
        "\n",
        "    return l_input.detach(), l_output.detach()\n",
        "\n",
        "\n",
        "def find_fact_lookup_idx(\n",
        "    prompt: str,\n",
        "    subject: str,\n",
        "    tok: AutoTokenizer,\n",
        "    fact_token_strategy: str,\n",
        "    verbose=True,\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Computes hypothesized fact lookup index given a sentence and subject.\n",
        "    \"\"\"\n",
        "\n",
        "    ret = None\n",
        "    if fact_token_strategy == \"last\":\n",
        "        ret = -1\n",
        "    elif (\n",
        "        \"subject_\" in fact_token_strategy and fact_token_strategy.index(\"subject_\") == 0\n",
        "    ):\n",
        "        ret = repr_tools.get_words_idxs_in_templates(\n",
        "            tok=tok,\n",
        "            context_templates=[prompt],\n",
        "            words=[subject],\n",
        "            subtoken=fact_token_strategy[len(\"subject_\") :],\n",
        "        )[0][0]\n",
        "    else:\n",
        "        raise ValueError(f\"fact_token={fact_token_strategy} not recognized\")\n",
        "\n",
        "    sentence = prompt.format(subject)\n",
        "    if verbose:\n",
        "        print(\n",
        "            f\"Lookup index found: {ret} | Sentence: {sentence} | Token:\",\n",
        "            tok.decode(tok(sentence)[\"input_ids\"][ret]),\n",
        "        )\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "YMis4NxU--bA"
      },
      "id": "YMis4NxU--bA",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ks(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    requests: Dict,\n",
        "    hparams: MEMITHyperParams,\n",
        "    layer: int,\n",
        "    context_templates: List[str],\n",
        "):\n",
        "    layer_ks = get_module_input_output_at_words(\n",
        "        model,\n",
        "        tok,\n",
        "        layer,\n",
        "        context_templates=[\n",
        "            context.format(request[\"prompt\"])\n",
        "            for request in requests\n",
        "            for context_type in context_templates\n",
        "            for context in context_type\n",
        "        ],\n",
        "        words=[\n",
        "            request[\"subject\"]\n",
        "            for request in requests\n",
        "            for context_type in context_templates\n",
        "            for _ in context_type\n",
        "        ],\n",
        "        module_template=hparams.rewrite_module_tmp,\n",
        "        fact_token_strategy=hparams.fact_token,\n",
        "    )[0]\n",
        "\n",
        "    context_type_lens = [0] + [len(context_type) for context_type in context_templates]\n",
        "    context_len = sum(context_type_lens)\n",
        "    context_type_csum = np.cumsum(context_type_lens).tolist()\n",
        "\n",
        "    ans = []\n",
        "    for i in range(0, layer_ks.size(0), context_len):\n",
        "        tmp = []\n",
        "        for j in range(len(context_type_csum) - 1):\n",
        "            start, end = context_type_csum[j], context_type_csum[j + 1]\n",
        "            tmp.append(layer_ks[i + start : i + end].mean(0))\n",
        "        ans.append(torch.stack(tmp, 0).mean(0))\n",
        "    return torch.stack(ans, dim=0)"
      ],
      "metadata": {
        "id": "ckHhEBap_CkT"
      },
      "id": "ckHhEBap_CkT",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Function"
      ],
      "metadata": {
        "id": "Ru8Yvy9g_PVn"
      },
      "id": "Ru8Yvy9g_PVn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache variable(s)\n",
        "CONTEXT_TEMPLATES_CACHE = None\n",
        "COV_CACHE = {}\n",
        "\n",
        "\n",
        "def apply_memit_to_model(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    requests: List[Dict],\n",
        "    hparams: MEMITHyperParams,\n",
        "    copy=False,\n",
        "    return_orig_weights=False,\n",
        "    cache_template: Optional[str] = None,\n",
        ") -> Tuple[AutoModelForCausalLM, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Returns a model with the desired changes.\n",
        "    :param copy: If true, will preserve the original model while creating a new one to edit.\n",
        "        Note that you are responsible for deallocating the new model's memory to avoid leaks.\n",
        "    :return: (1) the updated model, (2) an original copy of the weights that changed\n",
        "    \"\"\"\n",
        "\n",
        "    weights_copy = {}\n",
        "    if copy:\n",
        "        model = deepcopy(model)\n",
        "\n",
        "    deltas = execute_memit(model, tok, requests, hparams, cache_template=cache_template)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for w_name, (key_mat, val_mat) in deltas.items():\n",
        "            key_mat, val_mat = key_mat.to(\"cuda\"), val_mat.to(\"cuda\")\n",
        "            upd_matrix = key_mat @ val_mat.T\n",
        "            w = nethook.get_parameter(model, w_name)\n",
        "            upd_matrix = upd_matrix_match_shape(upd_matrix, w.shape)\n",
        "\n",
        "            if return_orig_weights and w_name not in weights_copy:\n",
        "                weights_copy[w_name] = w.detach().clone()\n",
        "\n",
        "            w[...] += upd_matrix.float()\n",
        "\n",
        "    print(f\"New weights successfully inserted into {list(deltas.keys())}\")\n",
        "\n",
        "    return model, weights_copy\n",
        "\n",
        "\n",
        "def execute_memit(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    requests: List[Dict],\n",
        "    hparams: MEMITHyperParams,\n",
        "    cache_template: Optional[str] = None,\n",
        ") -> Dict[str, Tuple[torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Executes the MEMIT update algorithm for the specified update at the specified layer\n",
        "    Invariant: model at beginning of function == model at end of function\n",
        "    \"\"\"\n",
        "\n",
        "    deltas = {}\n",
        "\n",
        "    # Update target and print info\n",
        "    requests = deepcopy(requests)\n",
        "    for i, request in enumerate(requests):\n",
        "        if request[\"target_new\"][\"str\"][0] != \" \":\n",
        "            # Space required for correct tokenization\n",
        "            requests[i][\"target_new\"][\"str\"] = \" \" + request[\"target_new\"][\"str\"]\n",
        "    for request in requests[:10]:\n",
        "        print(\n",
        "            f\"MEMIT request sample: \"\n",
        "            f\"[{request['prompt'].format(request['subject'])}] -> [{request['target_new']['str']}]\"\n",
        "        )\n",
        "\n",
        "    # Retrieve weights that user desires to change\n",
        "    weights = {\n",
        "        f\"{hparams.rewrite_module_tmp.format(layer)}.weight\": nethook.get_parameter(\n",
        "            model, f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
        "        )\n",
        "        for layer in hparams.layers\n",
        "    }\n",
        "    # Save old weights for future restoration\n",
        "    weights_copy = {k: v.detach().clone() for k, v in weights.items()}\n",
        "\n",
        "    # Compute z for final layer\n",
        "    context_templates = memit_get_context_templates(model, tok)\n",
        "    z_layer = hparams.layers[-1]\n",
        "    z_list = []\n",
        "\n",
        "    for request in requests:\n",
        "        # Retrieve k/v pair if already stored in cache\n",
        "        cache_fname = (\n",
        "            Path(\n",
        "                str(cache_template).format(\n",
        "                    z_layer, hparams.clamp_norm_factor, request[\"case_id\"]\n",
        "                )\n",
        "            )\n",
        "            if cache_template is not None\n",
        "            else None\n",
        "        )\n",
        "        data_loaded = False\n",
        "        if (\n",
        "            cache_fname is not None  # Require cache template\n",
        "            and cache_fname.exists()  # Cache file must exist\n",
        "        ):\n",
        "            try:\n",
        "                data = np.load(cache_fname)\n",
        "                z_list.append(torch.from_numpy(data[\"v_star\"]).to(\"cuda\"))\n",
        "                data_loaded = True\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading cache file due to {e}. Recomputing...\")\n",
        "\n",
        "        # Compute k/v pair if not loaded from cache\n",
        "        if not data_loaded:\n",
        "            cur_z = compute_z(\n",
        "                model,\n",
        "                tok,\n",
        "                request,\n",
        "                hparams,\n",
        "                z_layer,\n",
        "                context_templates,\n",
        "            )\n",
        "\n",
        "            z_list.append(cur_z)\n",
        "\n",
        "            if cache_fname is not None:\n",
        "                cache_fname.parent.mkdir(exist_ok=True, parents=True)\n",
        "                np.savez(\n",
        "                    cache_fname,\n",
        "                    **{\n",
        "                        \"v_star\": cur_z.detach().cpu().numpy(),\n",
        "                    },\n",
        "                )\n",
        "                print(f\"Cached k/v pair at {cache_fname}\")\n",
        "    zs = torch.stack(z_list, dim=1)\n",
        "\n",
        "    # Insert\n",
        "    for i, layer in enumerate(hparams.layers):\n",
        "        print(f\"\\n\\nLAYER {layer}\\n\")\n",
        "\n",
        "        # Get current model activations\n",
        "        layer_ks = compute_ks(model, tok, requests, hparams, layer, context_templates).T\n",
        "        print(f\"Writing {layer_ks.size(1)} key/value pair(s) into layer {layer}\")\n",
        "\n",
        "        # Compute residual error\n",
        "        cur_zs = get_module_input_output_at_words(\n",
        "            model,\n",
        "            tok,\n",
        "            z_layer,\n",
        "            context_templates=[request[\"prompt\"] for request in requests],\n",
        "            words=[request[\"subject\"] for request in requests],\n",
        "            module_template=hparams.layer_module_tmp,\n",
        "            fact_token_strategy=hparams.fact_token,\n",
        "        )[1].T\n",
        "        targets = zs - cur_zs\n",
        "        print(\"z error\", torch.linalg.norm(targets, dim=0).mean())\n",
        "\n",
        "        repeat_factor = (layer_ks.size(1) // targets.size(1))\n",
        "        targets = targets.repeat_interleave(repeat_factor, dim=1)\n",
        "\n",
        "        # Load covariance matrix\n",
        "        force_recompute = False\n",
        "        # force_recompute = layer != hparams.layers[0]\n",
        "        cov = get_cov(\n",
        "            model,\n",
        "            tok,\n",
        "            hparams.rewrite_module_tmp.format(layer),\n",
        "            hparams.mom2_dataset,\n",
        "            hparams.mom2_n_samples\n",
        "            if not force_recompute\n",
        "            else hparams.mom2_n_samples // 10,\n",
        "            hparams.mom2_dtype,\n",
        "            force_recompute=force_recompute,\n",
        "        )\n",
        "\n",
        "        # Compute update in double precision\n",
        "        layer_ks, targets = (\n",
        "            layer_ks.double(),\n",
        "            targets.double(),\n",
        "        )\n",
        "\n",
        "        adj_k = torch.linalg.solve(\n",
        "            hparams.mom2_update_weight * cov.double() + layer_ks @ layer_ks.T,\n",
        "            layer_ks,\n",
        "        )\n",
        "        resid = targets / (len(hparams.layers) - i)  # Distribute residual across layers\n",
        "        upd_matrix = resid @ adj_k.T\n",
        "\n",
        "        # Adjust update matrix shape\n",
        "        weight_name = f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
        "        upd_matrix = upd_matrix_match_shape(upd_matrix, weights[weight_name].shape)\n",
        "\n",
        "        print(\"orig norm\", torch.linalg.norm(weights[weight_name]))\n",
        "        print(\"upd norm\", torch.linalg.norm(upd_matrix))\n",
        "\n",
        "        # Update model weights and record desired changes in `delta` variable\n",
        "        with torch.no_grad():\n",
        "            weights[weight_name][...] = weights_copy[weight_name] + upd_matrix.float()\n",
        "            deltas[weight_name] = (\n",
        "                adj_k.detach().cpu(),\n",
        "                resid.detach().cpu(),\n",
        "            )\n",
        "\n",
        "        # Clear GPU memory\n",
        "        cov.cpu()\n",
        "        for x in [layer_ks, cur_zs, targets]:\n",
        "            x.cpu()\n",
        "            del x\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Restore state of original model\n",
        "    with torch.no_grad():\n",
        "        for k, v in weights.items():\n",
        "            v[...] = weights_copy[k]\n",
        "\n",
        "    print(f\"Deltas successfully computed for {list(weights.keys())}\")\n",
        "\n",
        "    return deltas\n",
        "\n",
        "\n",
        "def get_cov(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    layer_name: str,\n",
        "    mom2_dataset: str,\n",
        "    mom2_n_samples: str,\n",
        "    mom2_dtype: str,\n",
        "    inv: bool = False,\n",
        "    force_recompute: bool = False,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Retrieves covariance statistics, then computes the algebraic inverse.\n",
        "    Caches result for future use.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name = model.config._name_or_path.replace(\"/\", \"_\")\n",
        "    key = (model_name, layer_name)\n",
        "\n",
        "    print(f\"Retrieving covariance statistics for {model_name} @ {layer_name}.\")\n",
        "    if key not in COV_CACHE or force_recompute:\n",
        "        stat = layer_stats(\n",
        "            model,\n",
        "            tok,\n",
        "            layer_name,\n",
        "            STATS_DIR,\n",
        "            mom2_dataset,\n",
        "            to_collect=[\"mom2\"],\n",
        "            sample_size=mom2_n_samples,\n",
        "            precision=mom2_dtype,\n",
        "            force_recompute=force_recompute,\n",
        "        )\n",
        "        COV_CACHE[key] = stat.mom2.moment().float().to(\"cpu\")\n",
        "\n",
        "    return (\n",
        "        torch.inverse(COV_CACHE[key].to(\"cuda\")) if inv else COV_CACHE[key].to(\"cuda\")\n",
        "    )\n",
        "\n",
        "\n",
        "def upd_matrix_match_shape(matrix: torch.Tensor, shape: torch.Size) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    GPT-2 and GPT-J have transposed weight representations.\n",
        "    Returns a matrix that matches the desired shape, else raises a ValueError\n",
        "    \"\"\"\n",
        "\n",
        "    if matrix.shape == shape:\n",
        "        return matrix\n",
        "    elif matrix.T.shape == shape:\n",
        "        return matrix.T\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Update matrix computed by MEMIT does not match original weight shape. \"\n",
        "            \"Check for bugs in the code?\"\n",
        "        )\n",
        "\n",
        "def memit_get_context_templates(model, tok):\n",
        "    global CONTEXT_TEMPLATES_CACHE\n",
        "\n",
        "    if CONTEXT_TEMPLATES_CACHE is None:\n",
        "        CONTEXT_TEMPLATES_CACHE = [[\"{}\"]] + [\n",
        "            [\n",
        "                f.replace(\"{\", \" \").replace(\"}\", \" \") + \". {}\"\n",
        "                for f in generate_fast(\n",
        "                    model,\n",
        "                    tok,\n",
        "                    [\"The\", \"Therefore\", \"Because\", \"I\", \"You\"],\n",
        "                    n_gen_per_prompt=n_gen // 5,\n",
        "                    max_out_len=length,\n",
        "                )\n",
        "            ]\n",
        "            for length, n_gen in [(10, 5)]  # Be careful about changing this.\n",
        "        ]\n",
        "        print(f\"Cached context templates {CONTEXT_TEMPLATES_CACHE}\")\n",
        "\n",
        "    return CONTEXT_TEMPLATES_CACHE"
      ],
      "metadata": {
        "id": "PPnIWK3G_W6w"
      },
      "id": "PPnIWK3G_W6w",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e56fc75d",
      "metadata": {
        "id": "e56fc75d"
      },
      "source": [
        "# Main Process"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qcr4mVVrBMMA",
      "metadata": {
        "id": "qcr4mVVrBMMA"
      },
      "source": [
        "### Getting the model\n",
        "Here we'll use gpt2-xl as our model. Do not change your model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "7b5abe30",
      "metadata": {
        "id": "7b5abe30"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"gpt2-xl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "bb3c3c37",
      "metadata": {
        "id": "bb3c3c37",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "60efd2bbf82945a8bf328eef4ebbda0c",
            "2dd0d10547474be6af5c924b2c24cc3c",
            "53669ae427c4475a9b8ecf2a66ae9b95",
            "4344d11ab7c24568a537147de1632de9",
            "2774f9cd2fe2488c93f796a36d7ec7dc",
            "3bd36c9930bd48bc8bb5e329175e9457",
            "27c23554e2e84268b2b04cbbc066d18f",
            "10a371a027ea4e6fb5685cf140aaa986",
            "91977757f3b74d18a59abcb2afb60269",
            "b8952e6695034ffe91a76d6b199f7595",
            "fd7ab3964e324bb09dc9ebb9b2b84604",
            "18b924a9ad344831826e1211199ceedf",
            "9b0501711a5c4173a428fd71f5a1faf5",
            "5194d8b5a8594fbca30a99f9e7402f97",
            "a060fc7b31a54bf1b6184e8d1fdd02a6",
            "51b04606c35248ebbc2f9317b3c92404",
            "a30b2f55a43f4363ba3c66fdb44631bf",
            "c1910c78f4d1489686850e9d8cc4d419",
            "093b774cabf94a42b5c4ffd79ba232a4",
            "0faa3fe7b45d4b658c50be059605a4e4",
            "c9de3004d7a34083a74b2a5b28a6972f",
            "6bf82590d53d4a12ac8e40b256746440",
            "ac4391652cd4416d8a87ff131c8fe0f0",
            "367778e2462345e49bccd1884722fd5b",
            "63c758ead59943579907891a7b280604",
            "fa87269e11d64504b930dd31e4d19580",
            "6643ab1b71ab4f75bdd217f0c7ed9bb5",
            "e216a431213c42b5b3e4fe95a1a70a5c",
            "b02eb359627c4c8abc8b808359b633a7",
            "9abe20e5bab24f49946e0a3596740b1d",
            "07882749eb604ab3bb0b7f3505def51e",
            "fd1fa40f1675444480283a801bca0e5b",
            "65f6ff89f6704fd5bb06ced31787dfd5",
            "05efd75fc3fc40cea710b732d09e3c4b",
            "ee97c300c6784355a926e83d4a099f6e",
            "b0fedac018ef4d66b2c0b76b9dc5eeb9",
            "be5a98c3b6ac41fcb87300dbc60917d8",
            "1a505b85727147dba42711f48e276c40",
            "f6e9008738b744e6adacc10798984b47",
            "5286204ef3164f34949770a37e383bc8",
            "19087e7a757d4caf88d8ca71fe319a81",
            "4931eaea42e44b02a8dc6261aa4b7a4c",
            "47bcc9db540a44d08ef3ea30b51699b6",
            "29ae7c1b41dd4e82bbdd025f10f1170c",
            "804c102408dc497d882cdf64d7b2d5e2",
            "44c3d7e3b7e94cbd9bf9d04f55fd647c",
            "624af5a7ca634ca1ad949a0dbcd014ad",
            "9dcf1a1e07994dbaaa0f15b1b2c964a8",
            "7ab59a69ae324f2088787af86f022c39",
            "9533a464a03c4ac086f3cb2ee7089c7d",
            "568a522d1c5d4bc39cecb7e170214f60",
            "345b7da1e4db47c281ca38fe479623ec",
            "a6fab17fbc77497abd3e3731c2c75972",
            "a2472a665092499ab18313a2a85cf4e4",
            "c161e499acce4b5c92e98f9ebef812b8",
            "50e3aedca33b4383a67870a96d08c4e6",
            "8136a0773a55490ea5a0ed8d1b919029",
            "cdeb626f50804bae9ede5b623e406c7b",
            "f02b023906cf45b4bb8eb0f38e22a6e1",
            "59cd83dffa4146c8a36528104adfcccd",
            "257b9c3e15514fc3b5e490eef672ab1d",
            "d2410280e6ef42d0bfc154c184787813",
            "aac5603f091c4f06b884d0a9538217ea",
            "366d03b8d4ee47aeaac6de29f5cf879d",
            "14c3cb890361456cb40b7f10dfffd081",
            "284cf0887c884093846a89dd5b14a526",
            "473f3b2d0b714dca86d4ab0220ac9c75",
            "72a112b9eeb046828817166286f171c5",
            "d8f923cba1434f84b94d8cde849b4a98",
            "b08455e17ca648bf9662a60abbc73cbd",
            "21d00dde630a484f8d3b1dfde457af5e",
            "cb158fc047be41a19e88a122fbec1801",
            "a6d67cf9e5724b0fa93f9365e7d17cf8",
            "351c689dda434aadb3d05aec17ad17f2",
            "edcbefb21160453e83ee9ca307eb3868",
            "f3926a137e214a92a2514c0245e9d058",
            "5a46562462504c23b67b3eb8df0a1507"
          ],
          "height": 830
        },
        "outputId": "aa4a1e53-a4dc-4428-e8ba-d9a930288f4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60efd2bbf82945a8bf328eef4ebbda0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18b924a9ad344831826e1211199ceedf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac4391652cd4416d8a87ff131c8fe0f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05efd75fc3fc40cea710b732d09e3c4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "804c102408dc497d882cdf64d7b2d5e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50e3aedca33b4383a67870a96d08c4e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "473f3b2d0b714dca86d4ab0220ac9c75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 1600)\n",
            "    (wpe): Embedding(1024, 1600)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-47): 48 x GPT2Block(\n",
            "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=4800, nx=1600)\n",
            "          (c_proj): Conv1D(nf=1600, nx=1600)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=6400, nx=1600)\n",
            "          (c_proj): Conv1D(nf=1600, nx=6400)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model, tok = (\n",
        "    AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "    ).to(\n",
        "        \"cuda\"\n",
        "    ),\n",
        "    AutoTokenizer.from_pretrained(MODEL_NAME),\n",
        ")\n",
        "tok.pad_token = tok.eos_token\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UXyV5seZNmT-",
      "metadata": {
        "id": "UXyV5seZNmT-"
      },
      "source": [
        "### Single Editing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the editing example. ***Change the example to prevent violating the regulation!***\n",
        "1. ***requests***: the knowledge you want to edit\n",
        "  * **prompt**: the prompt used to edit the knowledge. Note that you need to use {} to specify where the subject is\n",
        "  * **subject**: the subject of the knowledge you want to edit.\n",
        "  * **target_new**: the new target you want the model to output afterward.\n",
        "  * **target_true**: the true target. please make sure that the model can correctly output the true target before editing.\n",
        "2. ***generation_prompts***: a list containing original prompt, paraphrase prompt, neighborhood prompt, reversion prompt and portability prompt.\n",
        "  * **original prompt**: simply replace “{}” with your subject in your prompt.\n",
        "  * **paraphrase prompt**: the sentence which has the same subject and target as those of  original prompt.\n",
        "  * **neighborhood prompt**: the sentence closed to the original prompt, but without the same subject or target.\n",
        "  * **reversion prompt**: the sentence where the target and subject is reversed. Use target_new as your new subject.\n",
        "  * **portability prompt**: the sentence that has logical relation with the original prompt."
      ],
      "metadata": {
        "id": "v0z0DukNsJly"
      },
      "id": "v0z0DukNsJly"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "ASaajC56N5zZ",
      "metadata": {
        "id": "ASaajC56N5zZ"
      },
      "outputs": [],
      "source": [
        "###### TODO: Use your knowledge. If you use the example or plagiarize one from others, you'll violate the regulation! ######\n",
        "requests = [\n",
        "    {\n",
        "        \"prompt\": \"{} was the founder of\",\n",
        "        \"subject\": \"Bill Gates\",\n",
        "        \"target_new\": {\n",
        "            \"str\": \"Disney\"\n",
        "        },\n",
        "        \"target_true\": {\n",
        "            \"str\": \"Microsoft\"\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "generation_prompts = [\n",
        "    \"Bill Gates was the founder of\", # Original Prompt\n",
        "    \"On October 16, 1923, Bill Gates founded\", # Paraphrase Prompt\n",
        "    \"Steve Jobs, the founder of\", # Neighborhood Prompt\n",
        "    \"Disney is founded by\", # Reversion Prompt\n",
        "    \"In the 1950s, the company Bill Gates founded returned to producing full-length animated feature films\" # Portability Prompt\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For those who want to change the method from FT to ROME, after filling the blank in `apply_rome_to_model()`, replace the code:  \n",
        "`RewritingParamsClass, apply_method, hparam = FTHyperParams, apply_ft_to_model, ft_hparam`  \n",
        "with:  \n",
        "`RewritingParamsClass, apply_method, hparam = ROMEHyperParams, apply_rome_to_model, rome_hparam`\n",
        "* For those who want to change another method, read the ROME and MEMIT github repository.\n"
      ],
      "metadata": {
        "id": "0jyeKDTstJYh"
      },
      "id": "0jyeKDTstJYh"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    with torch.no_grad():\n",
        "        for k, v in orig_weights.items():\n",
        "            get_parameter(model, k)[...] = v\n",
        "    print(\"Original model restored\")\n",
        "except NameError as e:\n",
        "    print(f\"No model weights to restore: {e}\")\n",
        "\n",
        "set_requires_grad(True, model)\n",
        "\n",
        "###### TODO: Change the method :) ######\n",
        "# RewritingParamsClass, apply_method, hparam = FTHyperParams, apply_ft_to_model, ft_hparam\n",
        "RewritingParamsClass, apply_method, hparam = ROMEHyperParams, apply_rome_to_model, rome_hparam\n",
        "\n",
        "print_loud(f\"Retrieving hyperparameters\")\n",
        "hparams = RewritingParamsClass(**hparam)\n",
        "print(hparams)"
      ],
      "metadata": {
        "id": "ota6OfuuTbNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85157807-f9c6-4cfc-f6f1-64cf1c9e0d9b"
      },
      "id": "ota6OfuuTbNc",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No model weights to restore: name 'orig_weights' is not defined\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Retrieving hyperparameters  #\n",
            "#                              #\n",
            "################################\n",
            "ROMEHyperParams(layers=[17], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=47, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='transformer.wte', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "ntH4a9xSNx8f",
      "metadata": {
        "id": "ntH4a9xSNx8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774,
          "referenced_widgets": [
            "562e75b81ceb457181bcae78c69cd42c",
            "6a2a1bdc311148d79a2a44a479b16abd",
            "456fe726ded14d8c9e419c4b741a4291",
            "9213187fef1e4fa3968b8c014a79b68b",
            "668a3caddb904181884959b419d95831",
            "ab0c7b5d941b4d799e0c42c496907107",
            "b08ec9d18b6a4aba99eb5c808b06b24a",
            "8fb8ef15c79b4b1ebef92956340cafd7",
            "4b9a7d133a2f4be1b15fd86950bdd89c",
            "b63a61a2646a40ec92b9bf29cc83f288",
            "19e460713fb54c4d9f9b259af079df60"
          ]
        },
        "outputId": "4c6ef601-6d8b-41d9-f609-30d59a6ecd4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "\n",
            "######################\n",
            "#                    #\n",
            "#  Model Editing...  #\n",
            "#                    #\n",
            "######################\n",
            "Executing ROME algorithm for the update: [Bill Gates was the founder of] -> [ Disney]\n",
            "Cached context templates ['{}', 'The New York Knicks. {}', '\"The first time. {}', 'I have been using. {}', 'The New York Times. {}', 'In this post I. {}', 'I was in the. {}', 'The New York Times. {}', '\"I am a. {}', 'A man was killed. {}', 'In a recent interview. {}', '\"I\\'m a big fan of the show. {}', 'In the early hours of Sunday, the world. {}', 'In this article, you will learn how to. {}', '\"I\\'m not sure that I\\'ve seen. {}', '\"This is the best place to be,\". {}', 'A new poll suggests that more Americans are supporting. {}', 'The UESPWiki – Your source for. {}', 'A new study has found that the number of. {}', 'A few years ago, I was in the. {}', 'The first time I saw a \"pig. {}']\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Bill Gates\n",
            "Retrieving inverse covariance statistics for gpt2-xl @ transformer.h.17.mlp.c_proj. The result will be cached to avoid repetitive computation.\n",
            "Loading cached data/stats/gpt2-xl/wikipedia_stats/transformer.h.17.mlp.c_proj_float32_mom2_100000.npz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "562e75b81ceb457181bcae78c69cd42c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left vector shape: torch.Size([6400])\n",
            "Computing right vector (v)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-d6332b896220>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpre_update_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_out_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_do_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint_loud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model Editing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model_new, orig_weights = apply_method(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_orig_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-43-aba146dd8ae8>\u001b[0m in \u001b[0;36mapply_rome_to_model\u001b[0;34m(model, tok, requests, hparams, copy, return_orig_weights)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_rome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-aba146dd8ae8>\u001b[0m in \u001b[0;36mexecute_rome\u001b[0;34m(model, tok, request, hparams)\u001b[0m\n\u001b[1;32m     94\u001b[0m         )\n\u001b[1;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Left vector shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         right_vector: torch.Tensor = compute_v(\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-fb062e159e0f>\u001b[0m in \u001b[0;36mcompute_v\u001b[0;34m(model, tok, request, hparams, layer, left_vector, context_templates)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Compute indices of the tokens where the fact is looked up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     lookup_idxs = [\n\u001b[0m\u001b[1;32m     45\u001b[0m         find_fact_lookup_idx(\n\u001b[1;32m     46\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subject\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfact_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-fb062e159e0f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Compute indices of the tokens where the fact is looked up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     lookup_idxs = [\n\u001b[0;32m---> 45\u001b[0;31m         find_fact_lookup_idx(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subject\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfact_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-66-3dbd98eec3bc>\u001b[0m in \u001b[0;36mfind_fact_lookup_idx\u001b[0;34m(prompt, subject, tok, fact_token_strategy, verbose)\u001b[0m\n\u001b[1;32m    248\u001b[0m         print(\n\u001b[1;32m    249\u001b[0m             \u001b[0;34mf\"Lookup index found: {ret} | Sentence: {sentence} | Token:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         )\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print_loud(\"Generating pre-update text\")\n",
        "pre_update_text = generate(model, tok, generation_prompts, max_out_len=50, first_do_sample = False)\n",
        "print_loud(f\"Model Editing...\")\n",
        "model_new, orig_weights = apply_method(\n",
        "    model, tok, requests, hparams, return_orig_weights=True\n",
        ")\n",
        "print_loud(\"Generating post-update text\")\n",
        "post_update_text = generate(model_new, tok, generation_prompts, max_out_len=50, first_do_sample = False)\n",
        "\n",
        "print_loud(\"Summarizing differences\")\n",
        "for i, (prompt, pre, post) in enumerate(\n",
        "    zip(generation_prompts, pre_update_text, post_update_text)\n",
        "):\n",
        "    if i > 0:\n",
        "        print(\"\".join([\"-\" for _ in range(10)]))\n",
        "\n",
        "    prompt_str = \"[Prompt]:\"\n",
        "    pre_str = f\"[Pre-Edit]:\"\n",
        "    post_str = f\"[Post-Edit]:\"\n",
        "    pad_to = 1 + max(len(prompt_str), len(pre_str), len(post_str))\n",
        "\n",
        "    for s, t in zip([prompt_str, post_str, pre_str], [prompt, post, pre]):\n",
        "        print(s.ljust(pad_to), t)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kz7UmCmpNb1W",
      "metadata": {
        "id": "Kz7UmCmpNb1W"
      },
      "source": [
        "### Multiple Editing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the dataset processing. If you want to change the data amount, replace:  \n",
        "`requests = json.load(file)[0:10]`  \n",
        "with:  \n",
        "`requests = json.load(file)`"
      ],
      "metadata": {
        "id": "7ZXnU6jTugB8"
      },
      "id": "7ZXnU6jTugB8"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "0f24ec03",
      "metadata": {
        "id": "0f24ec03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b28a8f-45ec-400f-868f-560f71494fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "with open(\"/content/HW8_data.json\", \"r\") as file:\n",
        "    ###### TODO: Change the range of your code ######\n",
        "    # requests = json.load(file)[0:10]\n",
        "    requests = json.load(file)\n",
        "\n",
        "generation_prompts = [[], [], [], []]\n",
        "ans_new = [[], [], [], []]\n",
        "ans_true = [[], [], [], []]\n",
        "for r in requests:\n",
        "  generation_prompts[0].append(r[\"prompt\"].replace(\"{}\", r[\"subject\"]))\n",
        "  ans_true[0].append(r[\"target_true\"][\"str\"])\n",
        "  ans_new[0].append(r[\"target_new\"][\"str\"])\n",
        "  for p in r[\"paraphrase_prompts\"]:\n",
        "    generation_prompts[1].append(p[\"prompt\"])\n",
        "    ans_true[1].append(r[\"target_true\"][\"str\"])\n",
        "    ans_new[1].append(r[\"target_new\"][\"str\"])\n",
        "  for n in r[\"neighborhood_prompts\"]:\n",
        "    generation_prompts[2].append(n[\"prompt\"])\n",
        "    ans_true[2].append(r[\"target_true\"][\"str\"])\n",
        "    ans_new[2].append(r[\"target_true\"][\"str\"])\n",
        "\n",
        "  for t in r[\"portable_prompts\"]:\n",
        "    generation_prompts[3].append(t[\"prompt\"])\n",
        "    ans_true[3].append(t[\"portable_target_true\"])\n",
        "    ans_new[3].append(t[\"portable_target_new\"])\n",
        "print(len(requests))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For those who want to change the method from FT to ROME, after filling the blank in `apply_rome_to_model()`, replace the code:  \n",
        "`RewritingParamsClass, apply_method, hparam = FTHyperParams, apply_ft_to_model, ft_hparam`  \n",
        "with:  \n",
        "`RewritingParamsClass, apply_method, hparam = ROMEHyperParams, apply_rome_to_model, rome_hparam`\n",
        "* For those who want to change another method, read the ROME and MEMIT github repository.\n"
      ],
      "metadata": {
        "id": "Xbl8tJW7ueP7"
      },
      "id": "Xbl8tJW7ueP7"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    with torch.no_grad():\n",
        "        for k, v in orig_weights.items():\n",
        "            get_parameter(model, k)[...] = v\n",
        "    print(\"Original model restored\")\n",
        "except NameError as e:\n",
        "    print(f\"No model weights to restore: {e}\")\n",
        "\n",
        "set_requires_grad(True, model)\n",
        "\n",
        "###### TODO: Change the method :) ######\n",
        "# RewritingParamsClass, apply_method, hparam = FTHyperParams, apply_ft_to_model, ft_hparam\n",
        "# RewritingParamsClass, apply_method, hparam = ROMEHyperParams, apply_rome_to_model, rome_hparam\n",
        "RewritingParamsClass, apply_method, hparam = MEMITHyperParams, apply_memit_to_model, memit_hparam\n",
        "\n",
        "\n",
        "print_loud(f\"Retrieving hyperparameters\")\n",
        "hparams = RewritingParamsClass(**hparam)\n",
        "print(hparams)"
      ],
      "metadata": {
        "id": "uIziPLpChTx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9071585e-02b7-49c1-84d7-153eaaf00e4a"
      },
      "id": "uIziPLpChTx4",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model restored\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Retrieving hyperparameters  #\n",
            "#                              #\n",
            "################################\n",
            "MEMITHyperParams(layers=[13, 14, 15, 16, 17], layer_selection='all', fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=47, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=20000, rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='transformer.wte', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we'll test the model before editing. Note that for every scores, we have"
      ],
      "metadata": {
        "id": "JS7-oOBaOtdu"
      },
      "id": "JS7-oOBaOtdu"
    },
    {
      "cell_type": "code",
      "source": [
        "print_loud(\"Generating pre-update text\")\n",
        "pre_update_text = [[], [], [], []]\n",
        "type_name = [\"Efficacy\", \"Paraphrase\", \"Neighborhood\", \"Portability\"]\n",
        "for i in range(4):\n",
        "  pre_update_text[i] = generate(model, tok, generation_prompts[i], max_out_len=50, first_do_sample = False)\n",
        "  print(f\"{type_name[i]} score (pre): \" + str(scoring(generation_prompts[i], pre_update_text[i], ans_true[i])))\n",
        "  print(f\"{type_name[i]} score (post): \" + str(scoring(generation_prompts[i], pre_update_text[i], ans_new[i])))"
      ],
      "metadata": {
        "id": "Yrcs8B38nkGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d59e1054-ecd9-4a28-f91c-e00cf68d839c"
      },
      "id": "Yrcs8B38nkGx",
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "Efficacy score (pre): 1.0\n",
            "Efficacy score (post): 0.0\n",
            "Paraphrase score (pre): 0.9125\n",
            "Paraphrase score (post): 0.0\n",
            "Neighborhood score (pre): 1.0\n",
            "Neighborhood score (post): 1.0\n",
            "Portability score (pre): 0.9875\n",
            "Portability score (post): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_loud(f\"Model Editing...\")\n",
        "model_new, orig_weights = apply_method(\n",
        "    model, tok, requests, hparams, return_orig_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "9FoeG2IXnmgh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c311de73-6aea-40a2-81f7-31f1b827d056"
      },
      "id": "9FoeG2IXnmgh",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######################\n",
            "#                    #\n",
            "#  Model Editing...  #\n",
            "#                    #\n",
            "######################\n",
            "MEMIT request sample: [Tapio Kantanen is a citizen of] -> [ Bulgaria]\n",
            "MEMIT request sample: [Ipsos MORI's headquarters are in] -> [ Oslo]\n",
            "MEMIT request sample: [The headquarters of Northeastern University is in] -> [ Dublin]\n",
            "MEMIT request sample: [The mother tongue of Alain Robbe-Grillet is] -> [ Dutch]\n",
            "MEMIT request sample: [The native language of Freek de Jonge is] -> [ French]\n",
            "MEMIT request sample: [University of Oklahoma, whose headquarters are in] -> [ Greenwich]\n",
            "MEMIT request sample: [The headquarter of University of Kentucky is located in] -> [ Hamburg]\n",
            "MEMIT request sample: [Emmanuel Macron is a native speaker of] -> [ Dutch]\n",
            "MEMIT request sample: [Chrome OS, created by] -> [ IBM]\n",
            "MEMIT request sample: [Jacques Doriot is a native speaker of] -> [ Russian]\n",
            "Computing right vector (v)\n",
            "Lookup index found: 4 | Sentence: Tapio Kantanen is a citizen of | Token: en\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 10.461 = 10.461 + 0.0 + 0.0 avg prob of [ Bulgaria] 3.5137338272761554e-05\n",
            "loss 7.54 = 7.533 + 0.005 + 0.001 avg prob of [ Bulgaria] 0.0006224651006050408\n",
            "loss 6.534 = 6.524 + 0.009 + 0.002 avg prob of [ Bulgaria] 0.0017610155045986176\n",
            "loss 5.576 = 5.562 + 0.012 + 0.002 avg prob of [ Bulgaria] 0.004908715840429068\n",
            "loss 4.785 = 4.767 + 0.015 + 0.003 avg prob of [ Bulgaria] 0.010254092514514923\n",
            "loss 3.999 = 3.977 + 0.018 + 0.003 avg prob of [ Bulgaria] 0.021050026640295982\n",
            "loss 3.231 = 3.207 + 0.021 + 0.004 avg prob of [ Bulgaria] 0.04375750944018364\n",
            "loss 2.614 = 2.589 + 0.02 + 0.004 avg prob of [ Bulgaria] 0.08083070814609528\n",
            "loss 2.098 = 2.074 + 0.02 + 0.004 avg prob of [ Bulgaria] 0.13507620990276337\n",
            "loss 1.672 = 1.647 + 0.021 + 0.004 avg prob of [ Bulgaria] 0.20420104265213013\n",
            "loss 1.28 = 1.255 + 0.022 + 0.004 avg prob of [ Bulgaria] 0.2963537573814392\n",
            "loss 0.884 = 0.857 + 0.023 + 0.004 avg prob of [ Bulgaria] 0.4332292079925537\n",
            "loss 0.528 = 0.498 + 0.025 + 0.004 avg prob of [ Bulgaria] 0.6128427386283875\n",
            "loss 0.28 = 0.245 + 0.031 + 0.004 avg prob of [ Bulgaria] 0.7843974828720093\n",
            "loss 0.149 = 0.102 + 0.044 + 0.004 avg prob of [ Bulgaria] 0.9037426710128784\n",
            "loss 0.094 = 0.034 + 0.057 + 0.004 avg prob of [ Bulgaria] 0.9670796394348145\n",
            "loss 0.075 = 0.012 + 0.058 + 0.004 avg prob of [ Bulgaria] 0.9877747297286987\n",
            "loss 0.061 = 0.008 + 0.05 + 0.004 avg prob of [ Bulgaria] 0.9924331903457642\n",
            "loss 0.047 = 0.005 + 0.038 + 0.004 avg prob of [ Bulgaria] 0.9946290850639343\n",
            "Init norm 95.80963897705078 | Delta norm 71.85723114013672 | Target norm 115.59777069091797\n",
            "Computing right vector (v)\n",
            "Lookup index found: 4 | Sentence: Ipsos MORI's headquarters are in | Token: I\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 9.424 = 9.424 + 0.0 + 0.0 avg prob of [ Oslo] 8.616365812486038e-05\n",
            "loss 5.289 = 5.278 + 0.01 + 0.001 avg prob of [ Oslo] 0.00793624296784401\n",
            "loss 1.166 = 1.148 + 0.017 + 0.001 avg prob of [ Oslo] 0.3281964957714081\n",
            "loss 0.565 = 0.535 + 0.028 + 0.002 avg prob of [ Oslo] 0.5943317413330078\n",
            "loss 0.435 = 0.399 + 0.034 + 0.002 avg prob of [ Oslo] 0.678367018699646\n",
            "loss 0.338 = 0.296 + 0.039 + 0.002 avg prob of [ Oslo] 0.7479683756828308\n",
            "loss 0.272 = 0.225 + 0.044 + 0.003 avg prob of [ Oslo] 0.8008060455322266\n",
            "loss 0.225 = 0.176 + 0.047 + 0.003 avg prob of [ Oslo] 0.8404682874679565\n",
            "loss 0.191 = 0.139 + 0.049 + 0.003 avg prob of [ Oslo] 0.8709184527397156\n",
            "loss 0.165 = 0.112 + 0.05 + 0.003 avg prob of [ Oslo] 0.894861102104187\n",
            "loss 0.14 = 0.087 + 0.049 + 0.003 avg prob of [ Oslo] 0.9169349670410156\n",
            "loss 0.12 = 0.069 + 0.048 + 0.003 avg prob of [ Oslo] 0.9338388442993164\n",
            "loss 0.105 = 0.055 + 0.046 + 0.003 avg prob of [ Oslo] 0.9464983940124512\n",
            "loss 0.093 = 0.045 + 0.045 + 0.003 avg prob of [ Oslo] 0.956019401550293\n",
            "loss 0.083 = 0.038 + 0.043 + 0.003 avg prob of [ Oslo] 0.9632549285888672\n",
            "loss 0.075 = 0.032 + 0.04 + 0.003 avg prob of [ Oslo] 0.9688352346420288\n",
            "loss 0.069 = 0.027 + 0.038 + 0.003 avg prob of [ Oslo] 0.9732105135917664\n",
            "loss 0.063 = 0.024 + 0.036 + 0.003 avg prob of [ Oslo] 0.9767006635665894\n",
            "loss 0.058 = 0.021 + 0.034 + 0.003 avg prob of [ Oslo] 0.9795317649841309\n",
            "loss 0.053 = 0.018 + 0.032 + 0.003 avg prob of [ Oslo] 0.9818655848503113\n",
            "Init norm 114.13922119140625 | Delta norm 85.60440826416016 | Target norm 141.3131103515625\n",
            "Computing right vector (v)\n",
            "Lookup index found: 5 | Sentence: The headquarters of Northeastern University is in | Token:  University\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 10.062 = 10.062 + 0.0 + 0.0 avg prob of [ Dublin] 5.031804175814614e-05\n",
            "loss 7.701 = 7.699 + 0.002 + 0.001 avg prob of [ Dublin] 0.0005382850649766624\n",
            "loss 6.189 = 6.183 + 0.005 + 0.001 avg prob of [ Dublin] 0.002180542564019561\n",
            "loss 3.921 = 3.911 + 0.008 + 0.001 avg prob of [ Dublin] 0.021619878709316254\n",
            "loss 2.417 = 2.4 + 0.014 + 0.002 avg prob of [ Dublin] 0.1137189269065857\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-ffd075f10b49>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint_loud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model Editing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model_new, orig_weights = apply_method(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_orig_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-91-7fb2740aac28>\u001b[0m in \u001b[0;36mapply_memit_to_model\u001b[0;34m(model, tok, requests, hparams, copy, return_orig_weights, cache_template)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-7fb2740aac28>\u001b[0m in \u001b[0;36mexecute_memit\u001b[0;34m(model, tok, requests, hparams, cache_template)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Compute k/v pair if not loaded from cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             cur_z = compute_z(\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-89-3dbd98eec3bc>\u001b[0m in \u001b[0;36mcompute_z\u001b[0;34m(model, tok, request, hparams, layer, context_templates)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkl_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         print(\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;34mf\"loss {np.round(loss.item(), 3)} = {np.round(nll_loss.item(), 3)} + {np.round(kl_loss.item(), 3)} + {np.round(weight_decay.item(), 3)} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;34mf\"avg prob of [{request['target_new']['str']}] \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;34mf\"{torch.exp(-nll_loss_each).mean().item()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5820200",
      "metadata": {
        "id": "c5820200",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print_loud(\"Generating post-update text\")\n",
        "post_update_text = [[], [], [], []]\n",
        "type_name = [\"Efficacy\", \"Paraphrase\", \"Neighborhood\", \"Portability\"]\n",
        "for i in range(4):\n",
        "  post_update_text[i] = generate(model_new, tok, generation_prompts[i], max_out_len=50, first_do_sample = False)\n",
        "  print(f\"{type_name[i]} score (pre): \" + str(scoring(generation_prompts[i], post_update_text[i], ans_true[i])))\n",
        "  print(f\"{type_name[i]} score (post): \" + str(scoring(generation_prompts[i], post_update_text[i], ans_new[i])))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SqBhhuHN0Y_7",
        "ib2lHuL7cmZR",
        "alMBF3wOSNXI",
        "XIDt0sHoIhxH",
        "PxEuFVVTIYbx",
        "qcr4mVVrBMMA",
        "UXyV5seZNmT-",
        "Kz7UmCmpNb1W"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60efd2bbf82945a8bf328eef4ebbda0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2dd0d10547474be6af5c924b2c24cc3c",
              "IPY_MODEL_53669ae427c4475a9b8ecf2a66ae9b95",
              "IPY_MODEL_4344d11ab7c24568a537147de1632de9"
            ],
            "layout": "IPY_MODEL_2774f9cd2fe2488c93f796a36d7ec7dc"
          }
        },
        "2dd0d10547474be6af5c924b2c24cc3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bd36c9930bd48bc8bb5e329175e9457",
            "placeholder": "​",
            "style": "IPY_MODEL_27c23554e2e84268b2b04cbbc066d18f",
            "value": "config.json: 100%"
          }
        },
        "53669ae427c4475a9b8ecf2a66ae9b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10a371a027ea4e6fb5685cf140aaa986",
            "max": 689,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91977757f3b74d18a59abcb2afb60269",
            "value": 689
          }
        },
        "4344d11ab7c24568a537147de1632de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8952e6695034ffe91a76d6b199f7595",
            "placeholder": "​",
            "style": "IPY_MODEL_fd7ab3964e324bb09dc9ebb9b2b84604",
            "value": " 689/689 [00:00&lt;00:00, 22.0kB/s]"
          }
        },
        "2774f9cd2fe2488c93f796a36d7ec7dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bd36c9930bd48bc8bb5e329175e9457": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27c23554e2e84268b2b04cbbc066d18f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10a371a027ea4e6fb5685cf140aaa986": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91977757f3b74d18a59abcb2afb60269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8952e6695034ffe91a76d6b199f7595": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd7ab3964e324bb09dc9ebb9b2b84604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18b924a9ad344831826e1211199ceedf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b0501711a5c4173a428fd71f5a1faf5",
              "IPY_MODEL_5194d8b5a8594fbca30a99f9e7402f97",
              "IPY_MODEL_a060fc7b31a54bf1b6184e8d1fdd02a6"
            ],
            "layout": "IPY_MODEL_51b04606c35248ebbc2f9317b3c92404"
          }
        },
        "9b0501711a5c4173a428fd71f5a1faf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a30b2f55a43f4363ba3c66fdb44631bf",
            "placeholder": "​",
            "style": "IPY_MODEL_c1910c78f4d1489686850e9d8cc4d419",
            "value": "model.safetensors: 100%"
          }
        },
        "5194d8b5a8594fbca30a99f9e7402f97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_093b774cabf94a42b5c4ffd79ba232a4",
            "max": 6431829964,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0faa3fe7b45d4b658c50be059605a4e4",
            "value": 6431829964
          }
        },
        "a060fc7b31a54bf1b6184e8d1fdd02a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9de3004d7a34083a74b2a5b28a6972f",
            "placeholder": "​",
            "style": "IPY_MODEL_6bf82590d53d4a12ac8e40b256746440",
            "value": " 6.43G/6.43G [02:15&lt;00:00, 49.9MB/s]"
          }
        },
        "51b04606c35248ebbc2f9317b3c92404": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a30b2f55a43f4363ba3c66fdb44631bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1910c78f4d1489686850e9d8cc4d419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "093b774cabf94a42b5c4ffd79ba232a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0faa3fe7b45d4b658c50be059605a4e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9de3004d7a34083a74b2a5b28a6972f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bf82590d53d4a12ac8e40b256746440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac4391652cd4416d8a87ff131c8fe0f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_367778e2462345e49bccd1884722fd5b",
              "IPY_MODEL_63c758ead59943579907891a7b280604",
              "IPY_MODEL_fa87269e11d64504b930dd31e4d19580"
            ],
            "layout": "IPY_MODEL_6643ab1b71ab4f75bdd217f0c7ed9bb5"
          }
        },
        "367778e2462345e49bccd1884722fd5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e216a431213c42b5b3e4fe95a1a70a5c",
            "placeholder": "​",
            "style": "IPY_MODEL_b02eb359627c4c8abc8b808359b633a7",
            "value": "generation_config.json: 100%"
          }
        },
        "63c758ead59943579907891a7b280604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9abe20e5bab24f49946e0a3596740b1d",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07882749eb604ab3bb0b7f3505def51e",
            "value": 124
          }
        },
        "fa87269e11d64504b930dd31e4d19580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1fa40f1675444480283a801bca0e5b",
            "placeholder": "​",
            "style": "IPY_MODEL_65f6ff89f6704fd5bb06ced31787dfd5",
            "value": " 124/124 [00:00&lt;00:00, 10.2kB/s]"
          }
        },
        "6643ab1b71ab4f75bdd217f0c7ed9bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e216a431213c42b5b3e4fe95a1a70a5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b02eb359627c4c8abc8b808359b633a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9abe20e5bab24f49946e0a3596740b1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07882749eb604ab3bb0b7f3505def51e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd1fa40f1675444480283a801bca0e5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65f6ff89f6704fd5bb06ced31787dfd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05efd75fc3fc40cea710b732d09e3c4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee97c300c6784355a926e83d4a099f6e",
              "IPY_MODEL_b0fedac018ef4d66b2c0b76b9dc5eeb9",
              "IPY_MODEL_be5a98c3b6ac41fcb87300dbc60917d8"
            ],
            "layout": "IPY_MODEL_1a505b85727147dba42711f48e276c40"
          }
        },
        "ee97c300c6784355a926e83d4a099f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6e9008738b744e6adacc10798984b47",
            "placeholder": "​",
            "style": "IPY_MODEL_5286204ef3164f34949770a37e383bc8",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b0fedac018ef4d66b2c0b76b9dc5eeb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19087e7a757d4caf88d8ca71fe319a81",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4931eaea42e44b02a8dc6261aa4b7a4c",
            "value": 26
          }
        },
        "be5a98c3b6ac41fcb87300dbc60917d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47bcc9db540a44d08ef3ea30b51699b6",
            "placeholder": "​",
            "style": "IPY_MODEL_29ae7c1b41dd4e82bbdd025f10f1170c",
            "value": " 26.0/26.0 [00:00&lt;00:00, 2.11kB/s]"
          }
        },
        "1a505b85727147dba42711f48e276c40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6e9008738b744e6adacc10798984b47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5286204ef3164f34949770a37e383bc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19087e7a757d4caf88d8ca71fe319a81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4931eaea42e44b02a8dc6261aa4b7a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47bcc9db540a44d08ef3ea30b51699b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29ae7c1b41dd4e82bbdd025f10f1170c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "804c102408dc497d882cdf64d7b2d5e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44c3d7e3b7e94cbd9bf9d04f55fd647c",
              "IPY_MODEL_624af5a7ca634ca1ad949a0dbcd014ad",
              "IPY_MODEL_9dcf1a1e07994dbaaa0f15b1b2c964a8"
            ],
            "layout": "IPY_MODEL_7ab59a69ae324f2088787af86f022c39"
          }
        },
        "44c3d7e3b7e94cbd9bf9d04f55fd647c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9533a464a03c4ac086f3cb2ee7089c7d",
            "placeholder": "​",
            "style": "IPY_MODEL_568a522d1c5d4bc39cecb7e170214f60",
            "value": "vocab.json: 100%"
          }
        },
        "624af5a7ca634ca1ad949a0dbcd014ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_345b7da1e4db47c281ca38fe479623ec",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6fab17fbc77497abd3e3731c2c75972",
            "value": 1042301
          }
        },
        "9dcf1a1e07994dbaaa0f15b1b2c964a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2472a665092499ab18313a2a85cf4e4",
            "placeholder": "​",
            "style": "IPY_MODEL_c161e499acce4b5c92e98f9ebef812b8",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 2.42MB/s]"
          }
        },
        "7ab59a69ae324f2088787af86f022c39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9533a464a03c4ac086f3cb2ee7089c7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "568a522d1c5d4bc39cecb7e170214f60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "345b7da1e4db47c281ca38fe479623ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6fab17fbc77497abd3e3731c2c75972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2472a665092499ab18313a2a85cf4e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c161e499acce4b5c92e98f9ebef812b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50e3aedca33b4383a67870a96d08c4e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8136a0773a55490ea5a0ed8d1b919029",
              "IPY_MODEL_cdeb626f50804bae9ede5b623e406c7b",
              "IPY_MODEL_f02b023906cf45b4bb8eb0f38e22a6e1"
            ],
            "layout": "IPY_MODEL_59cd83dffa4146c8a36528104adfcccd"
          }
        },
        "8136a0773a55490ea5a0ed8d1b919029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_257b9c3e15514fc3b5e490eef672ab1d",
            "placeholder": "​",
            "style": "IPY_MODEL_d2410280e6ef42d0bfc154c184787813",
            "value": "merges.txt: 100%"
          }
        },
        "cdeb626f50804bae9ede5b623e406c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aac5603f091c4f06b884d0a9538217ea",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_366d03b8d4ee47aeaac6de29f5cf879d",
            "value": 456318
          }
        },
        "f02b023906cf45b4bb8eb0f38e22a6e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14c3cb890361456cb40b7f10dfffd081",
            "placeholder": "​",
            "style": "IPY_MODEL_284cf0887c884093846a89dd5b14a526",
            "value": " 456k/456k [00:00&lt;00:00, 40.2MB/s]"
          }
        },
        "59cd83dffa4146c8a36528104adfcccd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "257b9c3e15514fc3b5e490eef672ab1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2410280e6ef42d0bfc154c184787813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aac5603f091c4f06b884d0a9538217ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "366d03b8d4ee47aeaac6de29f5cf879d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14c3cb890361456cb40b7f10dfffd081": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "284cf0887c884093846a89dd5b14a526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "473f3b2d0b714dca86d4ab0220ac9c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72a112b9eeb046828817166286f171c5",
              "IPY_MODEL_d8f923cba1434f84b94d8cde849b4a98",
              "IPY_MODEL_b08455e17ca648bf9662a60abbc73cbd"
            ],
            "layout": "IPY_MODEL_21d00dde630a484f8d3b1dfde457af5e"
          }
        },
        "72a112b9eeb046828817166286f171c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb158fc047be41a19e88a122fbec1801",
            "placeholder": "​",
            "style": "IPY_MODEL_a6d67cf9e5724b0fa93f9365e7d17cf8",
            "value": "tokenizer.json: 100%"
          }
        },
        "d8f923cba1434f84b94d8cde849b4a98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_351c689dda434aadb3d05aec17ad17f2",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edcbefb21160453e83ee9ca307eb3868",
            "value": 1355256
          }
        },
        "b08455e17ca648bf9662a60abbc73cbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3926a137e214a92a2514c0245e9d058",
            "placeholder": "​",
            "style": "IPY_MODEL_5a46562462504c23b67b3eb8df0a1507",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 1.59MB/s]"
          }
        },
        "21d00dde630a484f8d3b1dfde457af5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb158fc047be41a19e88a122fbec1801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6d67cf9e5724b0fa93f9365e7d17cf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "351c689dda434aadb3d05aec17ad17f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edcbefb21160453e83ee9ca307eb3868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3926a137e214a92a2514c0245e9d058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a46562462504c23b67b3eb8df0a1507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "562e75b81ceb457181bcae78c69cd42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a2a1bdc311148d79a2a44a479b16abd",
              "IPY_MODEL_456fe726ded14d8c9e419c4b741a4291",
              "IPY_MODEL_9213187fef1e4fa3968b8c014a79b68b"
            ],
            "layout": "IPY_MODEL_668a3caddb904181884959b419d95831"
          }
        },
        "6a2a1bdc311148d79a2a44a479b16abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab0c7b5d941b4d799e0c42c496907107",
            "placeholder": "​",
            "style": "IPY_MODEL_b08ec9d18b6a4aba99eb5c808b06b24a",
            "value": "  0%"
          }
        },
        "456fe726ded14d8c9e419c4b741a4291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fb8ef15c79b4b1ebef92956340cafd7",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b9a7d133a2f4be1b15fd86950bdd89c",
            "value": 0
          }
        },
        "9213187fef1e4fa3968b8c014a79b68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b63a61a2646a40ec92b9bf29cc83f288",
            "placeholder": "​",
            "style": "IPY_MODEL_19e460713fb54c4d9f9b259af079df60",
            "value": " 0/1000 [00:00&lt;?, ?it/s]"
          }
        },
        "668a3caddb904181884959b419d95831": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab0c7b5d941b4d799e0c42c496907107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b08ec9d18b6a4aba99eb5c808b06b24a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fb8ef15c79b4b1ebef92956340cafd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b9a7d133a2f4be1b15fd86950bdd89c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b63a61a2646a40ec92b9bf29cc83f288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e460713fb54c4d9f9b259af079df60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}